{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Module 2.4] 인퍼런스 시간 벤치 마킹\n",
    "\n",
    "### 본 워크샵의 모든 노트북은 `conda_python3` 여기에서 작업 합니다.\n",
    "\n",
    "이 노트북은 아래와 같은 작업을 합니다.\n",
    "\n",
    "- 0. 개념 확인\n",
    "- 1. 환경 셋업\n",
    "- 2. 모델 아티펙트 다운로드 및 압축해제\n",
    "- 3. 추론 함수 로컬 테스트\n",
    "- 4. 로컬 엔드포인트 생성\n",
    "- 5. 로컬 추론\n",
    "- 6. 로컬 엔드 포인트 삭제\n",
    "\n",
    "\n",
    "### 참고: \n",
    "- 세이지 메이커 개발자 가이드 --> [추론을 위한 모델 배포](https://docs.aws.amazon.com/ko_kr/sagemaker/latest/dg/deploy-model.html)\n",
    "- 세이지 메이커 배포에 대한 웹비나 --> [Amazon SageMaker 기반 사전 훈련된 딥러닝 모델 손쉽게 배포하기 – 김대근:: AWS Innovate 2021](https://www.youtube.com/watch?v=ZdOcrLKow3I)\n",
    "- 세이지 메이커 호스팅 기본 컨셉 --> [SageMaker 호스팅 아키텍쳐](https://github.com/gonsoomoon-ml/SageMaker-Pipelines-Step-By-Step/blob/main/scratch/8.1.Deploy-Pipeline.ipynb)\n",
    "- 세이지 메이커로 파이토치 사용 --> [Use PyTorch with the SageMaker Python SDK](https://sagemaker.readthedocs.io/en/stable/frameworks/pytorch/using_pytorch.html)\n",
    "    \n",
    "---    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. 개념 확인"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 환경 셋업"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. 기본 세팅\n",
    "사용하는 패키지는 import 시점에 다시 재로딩 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "sys.path.append('./src')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "전 노트북에서 훈련 후의 아티펙트를 가져옵니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r artifact_path\n",
    "%store -r bucket\n",
    "%store -r prefix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. 배포 준비"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.1. 모델 아티펙트 확인\n",
    "이 워크샵에서 제공한 노트북을 실행하셨다고 하면 아래의 모델 아티펙트가 생성이 되었을 겁니다.\n",
    "artifact_path 에 해당 변수를 할당하셔서 사용하시면 됩니다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model artifact is assigend from :  s3://sagemaker-ap-northeast-2-057716757052/pytorch-training-2023-03-05-09-10-14-368/output/model.tar.gz\n"
     ]
    }
   ],
   "source": [
    "print(\"model artifact is assigend from : \", artifact_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.2. 추론을 위한  데이터 세트 로딩\n",
    "- 전부 데이터를 로딩할 필요가 없지만, 여기서는 기존에 사용한 함수를 이용하기 위해서 전체 데이터를 로드 합니다. \n",
    "    - 실제 데이터로 구현시에는 따로이 로드 함수를 사용하시기를 권장 합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import data_utils \n",
    "train_data, test_data, user_num ,item_num, train_mat = data_utils.load_all(test_num=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of batch_size:  256\n"
     ]
    }
   ],
   "source": [
    "class Params:\n",
    "    def __init__(self):\n",
    "        # self.epochs = 1        \n",
    "        self.num_ng = 4\n",
    "        self.batch_size = 256\n",
    "        self.test_num_ng = 99\n",
    "        self.factor_num = 32\n",
    "        self.num_layers = 3\n",
    "        self.dropout = 0.0\n",
    "        # self.lr = 0.001\n",
    "        self.top_k = 10\n",
    "        self.out = True\n",
    "        # self.gpu = \"0\"\n",
    "                        \n",
    "args = Params()\n",
    "print(\"# of batch_size: \", args.batch_size)\n",
    "\n",
    "import torch.utils.data as data\n",
    "\n",
    "test_dataset = data_utils.NCFData(\n",
    "\t\ttest_data, item_num, train_mat, 0, False)\n",
    "\n",
    "test_loader = data.DataLoader(test_dataset,\n",
    "\t\tbatch_size=args.test_num_ng+1, shuffle=False, num_workers=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 모델 아티펙트 다운로드 및 압축해제\n",
    "- 모델 아티펙트를 다운로드 합니다.\n",
    "- 다운로드 받은 모델 아티펙트의 압축을 해제하고 모델 가중치인 model.pth 파일을 얻습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_data_dir:  ./models/\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import config\n",
    "\n",
    "model_data_dir = config.model_path\n",
    "os.makedirs(model_data_dir, exist_ok=True)\n",
    "print(\"model_data_dir: \", model_data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://sagemaker-ap-northeast-2-057716757052/pytorch-training-2023-03-05-09-10-14-368/output/model.tar.gz\n",
      "./models/\n",
      "download: s3://sagemaker-ap-northeast-2-057716757052/pytorch-training-2023-03-05-09-10-14-368/output/model.tar.gz to models/model.tar.gz\n",
      "NeuMF-end.pth\n"
     ]
    }
   ],
   "source": [
    "%%sh -s {artifact_path} {model_data_dir}\n",
    "\n",
    "artifact_path=$1\n",
    "model_data_dir=$2\n",
    "\n",
    "echo $artifact_path\n",
    "echo $model_data_dir\n",
    "\n",
    "# 기존 데이터 삭제\n",
    "rm -rf $model_data_dir/*\n",
    "\n",
    "# 모델을 S3에서 로컬로 다운로드\n",
    "aws s3 cp $artifact_path $model_data_dir\n",
    "\n",
    "# 모델 다운로드 폴더로 이동\n",
    "cd $model_data_dir\n",
    "\n",
    "# 압축 해제\n",
    "tar -xvf model.tar.gz  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. 추론 함수 로컬 테스트\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. 추론시 사용할 모델 네트워크 설정 저장\n",
    "- 모델 네트워크를 생성시에 설정값을 model_config.json 로 저장함.\n",
    "- model_fn() 함수에서 모델 네트워크를 생성시에 사용 함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "src/model_config.json is saved\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'src/model_config.json'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "from common_utils import save_json, load_json\n",
    "\n",
    "model_config_dict = {\n",
    "    'user_num': str(user_num),\n",
    "    'item_num': str(item_num),\n",
    "    'factor_num' : str(args.factor_num),\n",
    "    'num_layers' : str(args.num_layers),\n",
    "    'dropout' : str(args.dropout),\n",
    "    'model_type': config.model\n",
    "}\n",
    "\n",
    "model_config_file = 'model_config.json'\n",
    "model_config_file_path = os.path.join('src', model_config_file)\n",
    "\n",
    "save_json(model_config_file_path, model_config_dict)\n",
    "# model_config_dict = load_json(model_config_file_path)    \n",
    "# model_config_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. 사용자 정의 inference code\n",
    "\n",
    "- 사용자 정의 inference 코드를 정의해서 사용하기 전에, 노트북에서 사전 테스트 및 디버깅을 하고 진행하면 빠르게 추론 개발을 할수 있습니다.\n",
    "\n",
    "\n",
    "- 디폴트 inference code (input_fn, model_fn, predict_fn, output_fn) 을 사용해도 되지만, 상황에 따라서는 사용자 정의가 필요할 수 있습니다. 디폴트 코드는 아래 링크를 참고 하세요.\n",
    "    - [Deploy PyTorch Models](https://sagemaker.readthedocs.io/en/stable/frameworks/pytorch/using_pytorch.html#deploy-pytorch-models)\n",
    "    - [디폴트 inference Code](https://github.com/aws/sagemaker-pytorch-inference-toolkit/blob/master/src/sagemaker_pytorch_serving_container/default_pytorch_inference_handler.py)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from inference import model_fn , input_fn, predict_fn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.1. model_fn 테스트\n",
    "- 훈련한 모델 아티펙트 파일이 정상적으로 모델에 로딩이 되는지를 확인 합니다.\n",
    "- 일반적인 모델 로딩 에러\n",
    "    - Torch Version(에: 1.6.0) 이 훈련시에 사용한 버전과 동일해야 합니다. 만약 다들 경우에 `RuntimeError: version_ <= kMaxSupportedFileFormatVersion INTERNAL ASSERT FAILED` 에러가 발생합니다.\n",
    "    - 훈련시에 사용한 리소스 (GPU or CPU)가 추론시에도 동일해야 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "######## Staring model_fn() ###############\n",
      "--> model_dir : ./models/\n",
      "model_config_path: :  /home/ec2-user/SageMaker/aws-ai-ml-workshop-kr/sagemaker/recommendation/Neural-Collaborative-Filtering-On-SageMaker/2_Inference/./src/model_config.json\n",
      "--> model network is loaded\n",
      "model_file_path: :  {model_file_path}\n",
      "---> ########## Failure loading a Model #######\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ec2-user/SageMaker/aws-ai-ml-workshop-kr/sagemaker/recommendation/Neural-Collaborative-Filtering-On-SageMaker/2_Inference/./src/inference.py\", line 85, in model_fn\n",
      "    inf_model.load_state_dict(torch.load(f))\n",
      "  File \"/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1604, in load_state_dict\n",
      "    raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n",
      "RuntimeError: Error(s) in loading state_dict for NCF:\n",
      "\tUnexpected key(s) in state_dict: \"linear.weight\", \"linear.bias\". \n",
      "\n"
     ]
    }
   ],
   "source": [
    "inf_model = model_fn(config.model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.2. input_fn 테스트\n",
    "- 추론시의 입력을 input_fn에 전달하고 결과값이 잘 나오는지를 확인 합니다.\n",
    "- 입력값의 형태에 따라서 input_fn이 변경이 필요할 수 있습니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "추론 테스트할 1개의 레코드를 가져옵니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "payload:  {'user': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'item': [25, 1064, 174, 2791, 3373, 269, 2678, 1902, 3641, 1216, 915, 3672, 2803, 2344, 986, 3217, 2824, 2598, 464, 2340, 1952, 1855, 1353, 1547, 3487, 3293, 1541, 2414, 2728, 340, 1421, 1963, 2545, 972, 487, 3463, 2727, 1135, 3135, 128, 175, 2423, 1974, 2515, 3278, 3079, 1527, 2182, 1018, 2800, 1830, 1539, 617, 247, 3448, 1699, 1420, 2487, 198, 811, 1010, 1423, 2840, 1770, 881, 1913, 1803, 1734, 3326, 1617, 224, 3352, 1869, 1182, 1331, 336, 2517, 1721, 3512, 3656, 273, 1026, 1991, 2190, 998, 3386, 3369, 185, 2822, 864, 2854, 3067, 58, 2551, 2333, 2688, 3703, 1300, 1924, 3118]}\n"
     ]
    }
   ],
   "source": [
    "for user, item, label in test_loader:   \n",
    "    user_np = user.detach().cpu().numpy()\n",
    "    item_np = item.detach().cpu().numpy()            \n",
    "    break\n",
    "payload = {'user':user_np.tolist(), 'item':item_np.tolist()}\n",
    "print(\"payload: \", payload)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#### input_fn starting ######\n",
      "content_type: application/json\n",
      "#### type of input data: <class 'str'>\n",
      "input_fn_payload:  [tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0], device='cuda:0'), tensor([  25, 1064,  174, 2791, 3373,  269, 2678, 1902, 3641, 1216,  915, 3672,\n",
      "        2803, 2344,  986, 3217, 2824, 2598,  464, 2340, 1952, 1855, 1353, 1547,\n",
      "        3487, 3293, 1541, 2414, 2728,  340, 1421, 1963, 2545,  972,  487, 3463,\n",
      "        2727, 1135, 3135,  128,  175, 2423, 1974, 2515, 3278, 3079, 1527, 2182,\n",
      "        1018, 2800, 1830, 1539,  617,  247, 3448, 1699, 1420, 2487,  198,  811,\n",
      "        1010, 1423, 2840, 1770,  881, 1913, 1803, 1734, 3326, 1617,  224, 3352,\n",
      "        1869, 1182, 1331,  336, 2517, 1721, 3512, 3656,  273, 1026, 1991, 2190,\n",
      "         998, 3386, 3369,  185, 2822,  864, 2854, 3067,   58, 2551, 2333, 2688,\n",
      "        3703, 1300, 1924, 3118], device='cuda:0')]\n"
     ]
    }
   ],
   "source": [
    "input_fn_payload = input_fn(json.dumps(payload), content_type='application/json')\n",
    "print(\"input_fn_payload: \", input_fn_payload)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.3. predict_fn 테스트\n",
    "- input_fn 의 결과값이 predict_fn으로 전달이 됩니다. 이에 대한 동작을 확인 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#### predict_fn starting ######\n",
      "#### type of input data: <class 'list'>\n",
      "recommends:  [128, 273, 25, 174, 58, 175, 198, 464, 487, 1064]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[128, 273, 25, 174, 58, 175, 198, 464, 487, 1064]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction = predict_fn(input_fn_payload, inf_model)\n",
    "#output = predict_fn(images, model)\n",
    "prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. 벤치 마킹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_input(input_example):\n",
    "    user = input_example[0]\n",
    "    item = input_example[1]    \n",
    "    \n",
    "    return user, item\n",
    "\n",
    "def load_model_on_accelerator(model):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    model.eval()    \n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 일부 샘플로 추론 시간 측정\n",
    "\n",
    "아래는 예시로서 추론 시간이 0.88 ms, 0.66 ms 걸린다는 것을 의미 합니다.\n",
    "```\n",
    "latencies:  [0.88191, 0.66829, 0.63276, 0.77081, 0.61989]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "latencies:  [0.88191, 0.66829, 0.63276, 0.77081, 0.61989]\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "latencies = []\n",
    "num_test = 5\n",
    "for _ in range(num_test):\n",
    "    model = load_model_on_accelerator(inf_model)\n",
    "    user, item = split_input(input_fn_payload)\n",
    "    start = time.time()\n",
    "    predictions = model(user, item)    \n",
    "    finish = time.time()\n",
    "    elapse_time = round((finish - start) * 1000, 5)\n",
    "    latencies.append(elapse_time)\n",
    "\n",
    "print(\"latencies: \", latencies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 벤치 마킹 (ml.p3.2xlarge)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 스펙\n",
    "    - instqance = ml.p3.2xlarge\n",
    "        - GPU - V100 1장\n",
    "    - throughput = inferences / duration\n",
    "    - inferences : 인퍼런스 개수\n",
    "    - duration : 벤치 마크가 총 수행된 초 (단위: second)\n",
    "    - Latency: P50, P95, P99 - 전체 추론 시간의 퍼센타일 정보\n",
    "        - (단위: milli-second)\n",
    " \n",
    " ```\n",
    "Batch Size:  1\n",
    "Batches:     1000\n",
    "Inferences:  1000\n",
    "Threads:     1\n",
    "Models:      1\n",
    "Duration:    0.594\n",
    "Throughput:  1682.111\n",
    "Latency P50: 0.585\n",
    "Latency P95: 0.624\n",
    "Latency P99: 0.637\n",
    " ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Size:  1\n",
      "Batches:     1000\n",
      "Inferences:  1000\n",
      "Threads:     1\n",
      "Models:      1\n",
      "Duration:    0.594\n",
      "Throughput:  1682.111\n",
      "Latency P50: 0.585\n",
      "Latency P95: 0.624\n",
      "Latency P99: 0.637\n"
     ]
    }
   ],
   "source": [
    "from benchmark_util import benchmark\n",
    "model = load_model_on_accelerator(inf_model)\n",
    "user, item = split_input(input_fn_payload)\n",
    "\n",
    "benchmark(model, user, item, n_models=1, n_threads=1, batches_per_thread=1000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "notice": "Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved.  Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
