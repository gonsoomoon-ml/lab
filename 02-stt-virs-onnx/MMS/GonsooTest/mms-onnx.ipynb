{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/miniconda3/envs/onnx-conda-py36/lib/python38.zip\n",
      "/home/ubuntu/miniconda3/envs/onnx-conda-py36/lib/python3.8\n",
      "/home/ubuntu/miniconda3/envs/onnx-conda-py36/lib/python3.8/lib-dynload\n",
      "\n",
      "/home/ubuntu/miniconda3/envs/onnx-conda-py36/lib/python3.8/site-packages\n",
      "/home/ubuntu/mms-onnx/MMS\n",
      "/home/ubuntu/mms-onnx/MMS\n",
      "/home/ubuntu/mms-onnx/MMS/vits\n"
     ]
    }
   ],
   "source": [
    "# import sys\n",
    "# import os\n",
    "# sys.path.append(os.getcwd())\n",
    "# print(os.getcwd())\n",
    "\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.expanduser('~/mms-onnx/MMS'))\n",
    "sys.path.append(os.path.expanduser('~/mms-onnx/MMS/vits'))\n",
    "# print(\"sys.path: \\n\", sys.path)\n",
    "for i in sys.path:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import os\n",
    "from typing import Any, Dict\n",
    "\n",
    "import onnx\n",
    "import torch\n",
    "from vits import commons, utils\n",
    "from vits.models import SynthesizerTrn\n",
    "\n",
    "class OnnxModel(torch.nn.Module):\n",
    "    def __init__(self, model: SynthesizerTrn):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x,\n",
    "        x_lengths,\n",
    "        noise_scale=0.667,\n",
    "        length_scale=1.0,\n",
    "        noise_scale_w=0.8,\n",
    "    ):\n",
    "        return self.model.infer(\n",
    "            x=x,\n",
    "            x_lengths=x_lengths,\n",
    "            noise_scale=noise_scale,\n",
    "            length_scale=length_scale,\n",
    "            noise_scale_w=noise_scale_w,\n",
    "        )[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_meta_data(filename: str, meta_data: Dict[str, Any]):\n",
    "    \"\"\"Add meta data to an ONNX model. It is changed in-place.\n",
    "\n",
    "    Args:\n",
    "      filename:\n",
    "        Filename of the ONNX model to be changed.\n",
    "      meta_data:\n",
    "        Key-value pairs.\n",
    "    \"\"\"\n",
    "    model = onnx.load(filename)\n",
    "    for key, value in meta_data.items():\n",
    "        meta = model.metadata_props.add()\n",
    "        meta.key = key\n",
    "        meta.value = str(value)\n",
    "\n",
    "    onnx.save(model, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_vocab():\n",
    "    return [\n",
    "        x.replace(\"\\n\", \"\") for x in open(\"vocab.txt\", encoding=\"utf-8\").readlines()\n",
    "    ]\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def main():\n",
    "    hps = utils.get_hparams_from_file(\"config.json\")\n",
    "    is_uroman = hps.data.training_files.split(\".\")[-1] == \"uroman\"\n",
    "    if is_uroman:\n",
    "        raise ValueError(\"We don't support uroman!\")\n",
    "\n",
    "    symbols = load_vocab()\n",
    "\n",
    "    # Now generate tokens.txt\n",
    "    all_upper_tokens = [i.upper() for i in symbols]\n",
    "    duplicate = set(\n",
    "        [\n",
    "            item\n",
    "            for item, count in collections.Counter(all_upper_tokens).items()\n",
    "            if count > 1\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    print(\"generate tokens.txt\")\n",
    "\n",
    "    with open(\"tokens.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "        for idx, token in enumerate(symbols):\n",
    "            f.write(f\"{token} {idx}\\n\")\n",
    "\n",
    "            # both upper case and lower case correspond to the same ID\n",
    "            if (\n",
    "                token.lower() != token.upper()\n",
    "                and len(token.upper()) == 1\n",
    "                and token.upper() not in duplicate\n",
    "            ):\n",
    "                f.write(f\"{token.upper()} {idx}\\n\")\n",
    "\n",
    "    net_g = SynthesizerTrn(\n",
    "        len(symbols),\n",
    "        hps.data.filter_length // 2 + 1,\n",
    "        hps.train.segment_size // hps.data.hop_length,\n",
    "        **hps.model,\n",
    "    )\n",
    "    net_g.cpu()\n",
    "    _ = net_g.eval()\n",
    "\n",
    "    _ = utils.load_checkpoint(\"G_100000.pth\", net_g, None)\n",
    "\n",
    "    model = OnnxModel(net_g)\n",
    "\n",
    "    # x = torch.randint(low=1, high=10, size=(50,), dtype=torch.int64)\n",
    "    # x = torch.randint(low=1, high=10, size=(2,), dtype=torch.int64)\n",
    "    x = torch.randint(low=1, high=10, size=(10,), dtype=torch.int64)\n",
    "    # x = torch.randint(low=1, high=10, size=(5,), dtype=torch.int64)\n",
    "    x = x.unsqueeze(0)\n",
    "\n",
    "    x_length = torch.tensor([x.shape[1]], dtype=torch.int64)\n",
    "    \n",
    "    print(\"x: \\n\", x)\n",
    "    print(\"x: \", x.shape)\n",
    "    print(\"x_length shape: \", x_length.shape)\n",
    "    print(\"x_length: \", x_length)\n",
    "    \n",
    "    \n",
    "    noise_scale = torch.tensor([1], dtype=torch.float32)\n",
    "    length_scale = torch.tensor([1], dtype=torch.float32)\n",
    "    noise_scale_w = torch.tensor([1], dtype=torch.float32)\n",
    "\n",
    "    opset_version = 13\n",
    "\n",
    "    filename = \"model.onnx\"\n",
    "\n",
    "    torch.onnx.export(\n",
    "        model,\n",
    "        (x, x_length, noise_scale, length_scale, noise_scale_w),\n",
    "        filename,\n",
    "        opset_version=opset_version,\n",
    "        input_names=[\n",
    "            \"x\",\n",
    "            \"x_length\",\n",
    "            \"noise_scale\",\n",
    "            \"length_scale\",\n",
    "            \"noise_scale_w\",\n",
    "        ],\n",
    "        output_names=[\"y\"],\n",
    "        dynamic_axes={\n",
    "            \"x\": {0: \"N\", 1: \"L\"},  # n_audio is also known as batch_size\n",
    "            \"x_length\": {0: \"N\"},\n",
    "            \"y\": {0: \"N\", 2: \"L\"},\n",
    "        },\n",
    "    )\n",
    "    meta_data = {\n",
    "        \"model_type\": \"vits\",\n",
    "        \"comment\": \"mms\",\n",
    "        \"url\": \"https://huggingface.co/facebook/mms-tts/tree/main\",\n",
    "        \"add_blank\": int(hps.data.add_blank),\n",
    "        \"language\": os.environ.get(\"language\", \"unknown\"),\n",
    "        \"frontend\": \"characters\",\n",
    "        \"n_speakers\": int(hps.data.n_speakers),\n",
    "        \"sample_rate\": hps.data.sampling_rate,\n",
    "    }\n",
    "    print(\"meta_data\", meta_data)\n",
    "    add_meta_data(filename=filename, meta_data=meta_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generate tokens.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:root:Loaded checkpoint 'G_100000.pth' (iteration 6251)\n",
      "x: \n",
      " tensor([[8, 2, 4, 9, 7, 8, 3, 8, 6, 4]])\n",
      "x:  torch.Size([1, 10])\n",
      "x_length shape:  torch.Size([1])\n",
      "x_length:  tensor([10])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/mms-onnx/MMS/vits/attentions.py:157: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  assert t_s == t_t, \"Relative attention is only available for self-attention.\"\n",
      "/home/ubuntu/mms-onnx/MMS/vits/attentions.py:202: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  pad_length = max(length - (self.window_size + 1), 0)\n",
      "/home/ubuntu/mms-onnx/MMS/vits/attentions.py:203: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  slice_start_position = max((self.window_size + 1) - length, 0)\n",
      "/home/ubuntu/mms-onnx/MMS/vits/attentions.py:205: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if pad_length > 0:\n",
      "/home/ubuntu/mms-onnx/MMS/vits/transforms.py:105: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if torch.min(inputs) < left or torch.max(inputs) > right:\n",
      "/home/ubuntu/mms-onnx/MMS/vits/transforms.py:110: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if min_bin_width * num_bins > 1.0:\n",
      "/home/ubuntu/mms-onnx/MMS/vits/transforms.py:112: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if min_bin_height * num_bins > 1.0:\n",
      "/home/ubuntu/mms-onnx/MMS/vits/transforms.py:164: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  assert (discriminant >= 0).all()\n",
      "/home/ubuntu/miniconda3/envs/onnx-conda-py36/lib/python3.8/site-packages/torch/onnx/_internal/jit_utils.py:258: UserWarning: Constant folding - Only steps=1 can be constant folded for opset >= 10 onnx::Slice op. Constant folding not applied. (Triggered internally at ../torch/csrc/jit/passes/onnx/constant_fold.cpp:179.)\n",
      "  _C._jit_pass_onnx_node_shape_type_inference(node, params_dict, opset_version)\n",
      "/home/ubuntu/miniconda3/envs/onnx-conda-py36/lib/python3.8/site-packages/torch/onnx/_internal/jit_utils.py:258: UserWarning: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (Triggered internally at ../torch/csrc/jit/passes/onnx/shape_type_inference.cpp:1884.)\n",
      "  _C._jit_pass_onnx_node_shape_type_inference(node, params_dict, opset_version)\n",
      "/home/ubuntu/miniconda3/envs/onnx-conda-py36/lib/python3.8/site-packages/torch/onnx/utils.py:687: UserWarning: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (Triggered internally at ../torch/csrc/jit/passes/onnx/shape_type_inference.cpp:1884.)\n",
      "  _C._jit_pass_onnx_graph_shape_type_inference(\n",
      "/home/ubuntu/miniconda3/envs/onnx-conda-py36/lib/python3.8/site-packages/torch/onnx/utils.py:687: UserWarning: Constant folding - Only steps=1 can be constant folded for opset >= 10 onnx::Slice op. Constant folding not applied. (Triggered internally at ../torch/csrc/jit/passes/onnx/constant_fold.cpp:179.)\n",
      "  _C._jit_pass_onnx_graph_shape_type_inference(\n",
      "/home/ubuntu/miniconda3/envs/onnx-conda-py36/lib/python3.8/site-packages/torch/onnx/utils.py:1178: UserWarning: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (Triggered internally at ../torch/csrc/jit/passes/onnx/shape_type_inference.cpp:1884.)\n",
      "  _C._jit_pass_onnx_graph_shape_type_inference(\n",
      "/home/ubuntu/miniconda3/envs/onnx-conda-py36/lib/python3.8/site-packages/torch/onnx/utils.py:1178: UserWarning: Constant folding - Only steps=1 can be constant folded for opset >= 10 onnx::Slice op. Constant folding not applied. (Triggered internally at ../torch/csrc/jit/passes/onnx/constant_fold.cpp:179.)\n",
      "  _C._jit_pass_onnx_graph_shape_type_inference(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "meta_data {'model_type': 'vits', 'comment': 'mms', 'url': 'https://huggingface.co/facebook/mms-tts/tree/main', 'add_blank': 1, 'language': 'unknown', 'frontend': 'characters', 'n_speakers': 0, 'sample_rate': 16000}\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "onnx-conda-py36",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
