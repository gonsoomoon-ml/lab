{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "1b87ba52",
            "metadata": {},
            "source": [
                "# AWS Neuron compilation on Yolov8\n",
                "\n",
                "This notebook shows how to compile Yolov8/Pytorch to AWS Inferentia (inf1 instances) using NeuronSDK.\n",
                "\n",
                "Reference: \n",
                "- Model Prediction with Ultralytics YOLO\n",
                "    - https://docs.ultralytics.com/modes/predict/"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "id": "678768fc",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "The autoreload extension is already loaded. To reload it, use:\n",
                        "  %reload_ext autoreload\n",
                        "/home/ubuntu/lab/03-yolo8-inf1/notebook\n"
                    ]
                }
            ],
            "source": [
                "%load_ext autoreload\n",
                "%autoreload 2\n",
                "\n",
                "import sys, os\n",
                "print(os.getcwd())\n",
                "sys.path.append(os.path.abspath(\"..\"))\n",
                "\n",
                "# for i in sys.path:\n",
                "#     print(i)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "49acc5c3",
            "metadata": {},
            "source": [
                "## 1. Neuron Compilation using  Native Neuron SDK"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "45fd47d8",
            "metadata": {},
            "source": [
                "### Load yolo8 model using ultralytics Lib"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "id": "ad6f3253",
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "/home/ubuntu/miniconda3/envs/yolo8-conda-py310/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
                        "  from .autonotebook import tqdm as notebook_tqdm\n"
                    ]
                }
            ],
            "source": [
                "from ultralytics import YOLO\n",
                "\n",
                "model = YOLO(\"../model/yolov8n.pt\", task=\"detect\")\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "81366877",
            "metadata": {},
            "source": [
                "### Compile pytorch model to neuron model\n",
                "- When having compilation for the first time if there is no neuron compilation file, it has the following error. After that, if you try one more, it successfully compile it.\n",
                "    - The values for attribute 'shape' do not match: torch.Size([]) != torch.Size([1, 8400]).\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "id": "ca0c11e6",
            "metadata": {},
            "outputs": [],
            "source": [
                "from utils.local_util import * "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "id": "422acc95",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "input example shape:  torch.Size([1, 3, 640, 640])\n",
                        "/home/ubuntu/lab/03-yolo8-inf1/model/traced_yolo8_model_neuron.pt is given\n",
                        "Loaded existing model from ../model/traced_yolo8_model_neuron.pt\n"
                    ]
                }
            ],
            "source": [
                "import torch\n",
                "import torch_neuron\n",
                "import os\n",
                "\n",
                "pt_model_path = '../model/yolov8n.pt'\n",
                "neuron_model_path = \"../model/traced_yolo8_model_neuron.pt\"\n",
                "\n",
                "# generate dummy input example\n",
                "batch_sizes = 1\n",
                "input_shape = (batch_sizes, 3, 640, 640)\n",
                "inputs_example = torch.ones(input_shape)  # or numpy array for TF, MX\n",
                "print(\"input example shape: \", inputs_example.shape)\n",
                "\n",
                "\n",
                "if os.path.exists(neuron_model_path):\n",
                "    # Load the existing model\n",
                "    neuron_model = load_neuron_model(neuron_model_path)\n",
                "    print(f\"Loaded existing model from {neuron_model_path}\")\n",
                "else:\n",
                "    # trace the model forward\n",
                "    neuron_model = torch_neuron.trace(model.model.eval(), inputs_example)\n",
                "    print(f\"Compile and Load model from pytorch model, {pt_model_path}, and neuron model, {neuron_model_path}\")\n",
                "    print(f\"Neuron model is saved at, {neuron_model_path}\")\n",
                "    save_neuron_model(model=neuron_model, path=neuron_model_path)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "f793e067",
            "metadata": {},
            "source": [
                "### Inference on neuron model"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "249fecd0",
            "metadata": {},
            "source": [
                "##### infereince on dummy data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "id": "8f814ff9",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "result_neuron:  2 , shape:  torch.Size([64, 84, 8400])\n"
                    ]
                }
            ],
            "source": [
                "# result_neuron = neuron_model(inputs_example)\n",
                "\n",
                "batch_sizes = 64\n",
                "input_shape = (batch_sizes, 3, 640, 640)\n",
                "inputs_example = torch.ones(input_shape)  # or numpy array for TF, MX\n",
                "result_neuron = neuron_model(inputs_example)\n",
                "\n",
                "print(\"result_neuron: \", len(result_neuron), \", shape: \", result_neuron[0].shape)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "7476db75",
            "metadata": {},
            "source": [
                "##### infereince on bus image and post_processing"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 18,
            "id": "fd3bf13b",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "preprocessed_image:  (1, 3, 640, 640)\n",
                        "original_size:  (1080, 810)\n",
                        "result_neuron:  2 , shape: torch.Size([1, 84, 8400])\n",
                        "(1, 84, 8400)\n"
                    ]
                },
                {
                    "data": {
                        "text/plain": [
                            "[{'class_id': 0,\n",
                            "  'class_name': 'person',\n",
                            "  'confidence': 0.8887587785720825,\n",
                            "  'box': [478.0, 226.0, 84.0, 296.0],\n",
                            "  'scale': 1.6875},\n",
                            " {'class_id': 0,\n",
                            "  'class_name': 'person',\n",
                            "  'confidence': 0.8807970881462097,\n",
                            "  'box': [210.75, 241.0, 72.5, 266.0],\n",
                            "  'scale': 1.6875},\n",
                            " {'class_id': 0,\n",
                            "  'class_name': 'person',\n",
                            "  'confidence': 0.8774768114089966,\n",
                            "  'box': [109.25, 236.0, 115.5, 300.0],\n",
                            "  'scale': 1.6875},\n",
                            " {'class_id': 5,\n",
                            "  'class_name': 'bus',\n",
                            "  'confidence': 0.8459424376487732,\n",
                            "  'box': [97.0, 137.0, 458.0, 322.0],\n",
                            "  'scale': 1.6875},\n",
                            " {'class_id': 0,\n",
                            "  'class_name': 'person',\n",
                            "  'confidence': 0.4234580993652344,\n",
                            "  'box': [79.875, 326.0, 34.25, 188.0],\n",
                            "  'scale': 1.6875}]"
                        ]
                    },
                    "execution_count": 18,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "import cv2\n",
                "import numpy as np\n",
                "from ultralytics import YOLO\n",
                "\n",
                "# convert image to numpy array which shapes, [1,3,640,640]\n",
                "image_path = \"../test_image/bus.jpg\"\n",
                "preprocessed_image, original_size = preprocess_image(image_path)\n",
                "\n",
                "print(\"preprocessed_image: \", preprocessed_image.shape)\n",
                "print(\"original_size: \", original_size)\n",
                "\n",
                "preprocessed_image_torch = torch.from_numpy(preprocessed_image)\n",
                "\n",
                "# inference on neuron model\n",
                "result_neuron = neuron_model(preprocessed_image_torch)\n",
                "print(\"result_neuron: \", len(result_neuron), \", shape:\", result_neuron[0].shape)\n",
                "\n",
                "# convert tensor to numpy array, [1,84,8400]\n",
                "result_np = result_neuron[0].numpy()\n",
                "print(result_np.shape)\n",
                "\n",
                "# post_process for showing bound box\n",
                "post_process_ultralytics(input_image=image_path, outputs=result_np)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "c0082e3a",
            "metadata": {},
            "source": [
                "## 2. Compile and inference using ultralytics lib"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "060f5dc9",
            "metadata": {},
            "source": [
                "### Load pytorch model, yolo8, and compile it to neuron model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 19,
            "id": "793ab87d",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Ultralytics YOLOv8.2.0 🚀 Python-3.10.15 torch-1.13.1+cu117 CPU (Intel Xeon Platinum 8275CL 3.00GHz)\n",
                        "YOLOv8n summary (fused): 168 layers, 3151904 parameters, 0 gradients, 8.7 GFLOPs\n",
                        "\n",
                        "\u001b[34m\u001b[1mPyTorch:\u001b[0m starting from '../model/yolov8n.pt' with input shape (1, 3, 640, 640) BCHW and output shape(s) (1, 84, 8400) (6.2 MB)\n",
                        "\n",
                        "\u001b[34m\u001b[1mNeuron:\u001b[0m starting export with torch_neuron 1.13.1.2.11.7.0 and neuron-cc 1.24.0.0+d58fa6134...\n",
                        "\u001b[34m\u001b[1mNeuron:\u001b[0m WARNING ⚠️ export may fail if requirement numpy<=1.21.6,>=1.20 does not satisfy\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "INFO:Neuron:All operators are compiled by neuron-cc (this does not guarantee that neuron-cc will successfully compile)\n",
                        "INFO:Neuron:Number of arithmetic operators (pre-compilation) before = 186, fused = 186, percent fused = 100.0%\n",
                        "INFO:Neuron:Compiler args type is <class 'list'> value is ['--fast-math', 'none']\n",
                        "INFO:Neuron:Compiling function _NeuronGraph$1524 with neuron-cc\n",
                        "INFO:Neuron:Compiling with command line: '/home/ubuntu/miniconda3/envs/yolo8-conda-py310/bin/neuron-cc compile /tmp/tmpg2lzlp3k/graph_def.pb --framework TENSORFLOW --pipeline compile SaveTemps --output /tmp/tmpg2lzlp3k/graph_def.neff --io-config {\"inputs\": {\"0:0\": [[1, 3, 640, 640], \"float32\"]}, \"outputs\": [\"Detect_74/aten_cat_5/concat:0\"]} --fast-math none --verbose 35'\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "....\n",
                        "Compiler status PASS\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "INFO:Neuron:Number of arithmetic operators (post-compilation) before = 186, compiled = 186, percent compiled = 100.0%\n",
                        "INFO:Neuron:The neuron partitioner created 1 sub-graphs\n",
                        "INFO:Neuron:Neuron successfully compiled 1 sub-graphs, Total fused subgraphs = 1, Percent of model sub-graphs successfully compiled = 100.0%\n",
                        "INFO:Neuron:Compiled these operators (and operator counts) to Neuron:\n",
                        "INFO:Neuron: => aten::Int: 7\n",
                        "INFO:Neuron: => aten::_convolution: 64\n",
                        "INFO:Neuron: => aten::add: 8\n",
                        "INFO:Neuron: => aten::cat: 19\n",
                        "INFO:Neuron: => aten::chunk: 1\n",
                        "INFO:Neuron: => aten::div: 1\n",
                        "INFO:Neuron: => aten::max_pool2d: 3\n",
                        "INFO:Neuron: => aten::mul: 1\n",
                        "INFO:Neuron: => aten::sigmoid: 1\n",
                        "INFO:Neuron: => aten::silu_: 57\n",
                        "INFO:Neuron: => aten::size: 3\n",
                        "INFO:Neuron: => aten::softmax: 1\n",
                        "INFO:Neuron: => aten::split_with_sizes: 9\n",
                        "INFO:Neuron: => aten::sub: 2\n",
                        "INFO:Neuron: => aten::transpose: 1\n",
                        "INFO:Neuron: => aten::unsqueeze: 1\n",
                        "INFO:Neuron: => aten::upsample_nearest2d: 2\n",
                        "INFO:Neuron: => aten::view: 5\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\u001b[34m\u001b[1mNeuron:\u001b[0m export success ✅ 84.6s, saved as '../model/yolov8n.neuron' (11.3 MB)\n",
                        "\n",
                        "Export complete (86.0s)\n",
                        "Results saved to \u001b[1m/home/ubuntu/lab/03-yolo8-inf1/model\u001b[0m\n",
                        "Predict:         yolo predict task=detect model=../model/yolov8n.neuron imgsz=640  \n",
                        "Validate:        yolo val task=detect model=../model/yolov8n.neuron imgsz=640 data=coco.yaml  \n",
                        "Visualize:       https://netron.app\n",
                        "Compile and Load model from pytorch model, ../model/yolov8n.pt, and neuron model, ../model/yolov8n.neuron\n"
                    ]
                }
            ],
            "source": [
                "from ultralytics import YOLO\n",
                "\n",
                "import os\n",
                "\n",
                "pt_model_path = '../model/yolov8n.pt'\n",
                "neuron_model_path = '../model/yolov8n.neuron'\n",
                "\n",
                "if os.path.exists(neuron_model_path):\n",
                "    # Load the existing model\n",
                "    # m_inf= YOLO(\"../model/traced_yolo8_model_neuron.pt\", task=\"detect\")\n",
                "    m_inf= YOLO(neuron_model_path, task=\"detect\")\n",
                "    print(f\"Loaded existing model from {neuron_model_path}\")\n",
                "else:\n",
                "    mx=YOLO(pt_model_path)\n",
                "    mx.export(format=\"neuron\")\n",
                "    # m_inf= YOLO(\"model/yolov8n.neuron\", task=\"detect\")\n",
                "    m_inf= YOLO(neuron_model_path, task=\"detect\")\n",
                "    print(f\"Compile and Load model from pytorch model, {pt_model_path}, and neuron model, {neuron_model_path}\")\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "7b1ca898",
            "metadata": {},
            "source": [
                "### inference on neuron model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 20,
            "id": "ca9c9eee",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Loading ../model/yolov8n.neuron for Neuron (NeuronCore-v1) inference...\n",
                        "\n",
                        "image 1/1 /home/ubuntu/lab/03-yolo8-inf1/notebook/../test_image/bus.jpg: 640x640 4 persons, 1 bus, 27.0ms\n",
                        "Speed: 2.1ms preprocess, 27.0ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
                        "Results saved to \u001b[1mresult_image/predict4\u001b[0m\n",
                        "1 label saved to result_image/predict4/labels\n"
                    ]
                }
            ],
            "source": [
                "results = m_inf.predict(\"../test_image/bus.jpg\", \n",
                "                            # show=True,\n",
                "                            save=True, \n",
                "                            save_txt=True, \n",
                "                            save_crop=True, \n",
                "                            save_conf=True,\n",
                "                            project='result_image')\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "f68ba52b",
            "metadata": {},
            "source": [
                "### Bounding Box information\n",
                "Refer to the link \n",
                "- [Model Prediction with Ultralytics YOLO](https://docs.ultralytics.com/modes/predict/#working-with-results)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "id": "181884e5",
            "metadata": {},
            "outputs": [
                {
                    "ename": "NameError",
                    "evalue": "name 'results' is not defined",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
                        "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
                        "Cell \u001b[0;32mIn[10], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# View results\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m \u001b[43mresults\u001b[49m:\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28mprint\u001b[39m(r\u001b[38;5;241m.\u001b[39mboxes)  \u001b[38;5;66;03m# print the Boxes object containing the detection bounding boxes\u001b[39;00m\n",
                        "\u001b[0;31mNameError\u001b[0m: name 'results' is not defined"
                    ]
                }
            ],
            "source": [
                "# View results\n",
                "for r in results:\n",
                "    print(r.boxes)  # print the Boxes object containing the detection bounding boxes"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "d824a28c",
            "metadata": {},
            "outputs": [],
            "source": []
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "yolo8-conda-py310",
            "language": "python",
            "name": "yolo8-conda-py310"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.15"
        },
        "vscode": {
            "interpreter": {
                "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
            }
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
