{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "1b87ba52",
            "metadata": {},
            "source": [
                "# AWS Neuron compilation on Yolov8\n",
                "\n",
                "This notebook shows how to compile Yolov8/Pytorch to AWS Inferentia (inf1 instances) using NeuronSDK.\n",
                "\n",
                "Reference: \n",
                "- Model Prediction with Ultralytics YOLO\n",
                "    - https://docs.ultralytics.com/modes/predict/"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "id": "678768fc",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "/home/ubuntu/lab/03-yolo8-inf1/notebook\n"
                    ]
                }
            ],
            "source": [
                "%load_ext autoreload\n",
                "%autoreload 2\n",
                "\n",
                "import sys, os\n",
                "print(os.getcwd())\n",
                "sys.path.append(os.path.abspath(\"..\"))\n",
                "\n",
                "# for i in sys.path:\n",
                "#     print(i)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "49acc5c3",
            "metadata": {},
            "source": [
                "## 1. Neuron Compilation using  Native Neuron SDK"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "45fd47d8",
            "metadata": {},
            "source": [
                "### Load yolo8 model using ultralytics Lib"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "id": "ad6f3253",
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "/home/ubuntu/miniconda3/envs/yolo8-conda-py310/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
                        "  from .autonotebook import tqdm as notebook_tqdm\n"
                    ]
                }
            ],
            "source": [
                "from ultralytics import YOLO\n",
                "\n",
                "model = YOLO(\"../model/yolov8n.pt\", task=\"detect\")\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "81366877",
            "metadata": {},
            "source": [
                "### Compile pytorch model to neuron model\n",
                "- When having an error, skip this cell"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "ca0c11e6",
            "metadata": {},
            "outputs": [],
            "source": [
                "from utils.local_util import * "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "id": "422acc95",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "input example shape:  torch.Size([1, 3, 640, 640])\n",
                        "/home/ubuntu/lab/03-yolo8-inf1/model/traced_yolo8_model_neuron.pt is given\n",
                        "Loaded existing model from ../model/traced_yolo8_model_neuron.pt\n"
                    ]
                }
            ],
            "source": [
                "import torch\n",
                "import torch_neuron\n",
                "import os\n",
                "\n",
                "pt_model_path = '../model/yolov8n.pt'\n",
                "neuron_model_path = \"../model/traced_yolo8_model_neuron.pt\"\n",
                "\n",
                "# generate dummy input example\n",
                "batch_sizes = 1\n",
                "input_shape = (batch_sizes, 3, 640, 640)\n",
                "inputs_example = torch.ones(input_shape)  # or numpy array for TF, MX\n",
                "print(\"input example shape: \", inputs_example.shape)\n",
                "\n",
                "\n",
                "if os.path.exists(neuron_model_path):\n",
                "    # Load the existing model\n",
                "    neuron_model = load_neuron_model(neuron_model_path)\n",
                "    print(f\"Loaded existing model from {neuron_model_path}\")\n",
                "else:\n",
                "    # trace the model forward\n",
                "    neuron_model = torch_neuron.trace(model.model.eval(), inputs_example)\n",
                "    print(f\"Compile and Load model from pytorch model, {pt_model_path}, and neuron model, {neuron_model_path}\")\n",
                "    print(f\"Neuron model is saved at, {neuron_model_path}\")\n",
                "    save_neuron_model(model=trace, path=neuron_model_path)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "f793e067",
            "metadata": {},
            "source": [
                "### Inference on neuron model"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "249fecd0",
            "metadata": {},
            "source": [
                "##### infereince on dummy data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "8f814ff9",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "result_neuron:  2 , shape:  torch.Size([1, 84, 8400])\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "/home/ubuntu/miniconda3/envs/yolo8-conda-py310/lib/python3.10/site-packages/torch_neuron/data_parallel.py:220: UserWarning: The NeuronCores are not being fully utilized because `inputs.shape[dim]` is not divisible by the number of NeuronCores given in `device_ids`. In order to get optimal performance, please try to ensure that the shape your inputs at `dim` is divisible by the number of NeuronCores that DataParallel is using, such that `input.shape[dim] % len(device_ids) == 0).`\n",
                        "  warnings.warn('The NeuronCores are not being fully utilized because '\n"
                    ]
                }
            ],
            "source": [
                "result_neuron = neuron_model(inputs_example)\n",
                "print(\"result_neuron: \", len(result_neuron), \", shape: \", result_neuron[0].shape)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "7476db75",
            "metadata": {},
            "source": [
                "##### infereince on bus image and post_processing"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "id": "fd3bf13b",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "preprocessed_image:  (1, 3, 640, 640)\n",
                        "original_size:  (1080, 810)\n",
                        "result_neuron:  2 , shape: torch.Size([1, 84, 8400])\n",
                        "(1, 84, 8400)\n"
                    ]
                },
                {
                    "data": {
                        "text/plain": [
                            "[{'class_id': 0,\n",
                            "  'class_name': 'person',\n",
                            "  'confidence': 0.8887587785720825,\n",
                            "  'box': [478.0, 226.0, 84.0, 296.0],\n",
                            "  'scale': 1.6875},\n",
                            " {'class_id': 0,\n",
                            "  'class_name': 'person',\n",
                            "  'confidence': 0.8807970881462097,\n",
                            "  'box': [210.75, 241.0, 72.5, 266.0],\n",
                            "  'scale': 1.6875},\n",
                            " {'class_id': 0,\n",
                            "  'class_name': 'person',\n",
                            "  'confidence': 0.8774768114089966,\n",
                            "  'box': [109.25, 236.0, 115.5, 300.0],\n",
                            "  'scale': 1.6875},\n",
                            " {'class_id': 5,\n",
                            "  'class_name': 'bus',\n",
                            "  'confidence': 0.8459424376487732,\n",
                            "  'box': [97.0, 137.0, 458.0, 322.0],\n",
                            "  'scale': 1.6875},\n",
                            " {'class_id': 0,\n",
                            "  'class_name': 'person',\n",
                            "  'confidence': 0.4234580993652344,\n",
                            "  'box': [79.875, 326.0, 34.25, 188.0],\n",
                            "  'scale': 1.6875}]"
                        ]
                    },
                    "execution_count": 6,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "import cv2\n",
                "import numpy as np\n",
                "from ultralytics import YOLO\n",
                "\n",
                "# convert image to numpy array which shapes, [1,3,640,640]\n",
                "image_path = \"../test_image/bus.jpg\"\n",
                "preprocessed_image, original_size = preprocess_image(image_path)\n",
                "\n",
                "print(\"preprocessed_image: \", preprocessed_image.shape)\n",
                "print(\"original_size: \", original_size)\n",
                "\n",
                "preprocessed_image_torch = torch.from_numpy(preprocessed_image)\n",
                "\n",
                "# inference on neuron model\n",
                "result_neuron = neuron_model(preprocessed_image_torch)\n",
                "print(\"result_neuron: \", len(result_neuron), \", shape:\", result_neuron[0].shape)\n",
                "\n",
                "# convert tensor to numpy array, [1,84,8400]\n",
                "result_np = result_neuron[0].numpy()\n",
                "print(result_np.shape)\n",
                "\n",
                "# post_process for showing bound box\n",
                "post_process_ultralytics(input_image=image_path, outputs=result_np)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "c0082e3a",
            "metadata": {},
            "source": [
                "## 2. Compile and inference using ultralytics lib"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "060f5dc9",
            "metadata": {},
            "source": [
                "### Load pytorch model, yolo8, and compile it to neuron model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "id": "793ab87d",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Loaded existing model from ../model/yolov8n.neuron\n"
                    ]
                }
            ],
            "source": [
                "from ultralytics import YOLO\n",
                "\n",
                "import os\n",
                "\n",
                "pt_model_path = '../model/yolov8n.pt'\n",
                "neuron_model_path = '../model/yolov8n.neuron'\n",
                "\n",
                "if os.path.exists(neuron_model_path):\n",
                "    # Load the existing model\n",
                "    # m_inf= YOLO(\"../model/traced_yolo8_model_neuron.pt\", task=\"detect\")\n",
                "    m_inf= YOLO(neuron_model_path, task=\"detect\")\n",
                "    print(f\"Loaded existing model from {neuron_model_path}\")\n",
                "else:\n",
                "    mx=YOLO(pt_model_path)\n",
                "    mx.export(format=\"neuron\")\n",
                "    # m_inf= YOLO(\"model/yolov8n.neuron\", task=\"detect\")\n",
                "    m_inf= YOLO(neuron_model_path, task=\"detect\")\n",
                "    print(f\"Compile and Load model from pytorch model, {pt_model_path}, and neuron model, {neuron_model_path}\")\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "7b1ca898",
            "metadata": {},
            "source": [
                "### inference on neuron model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "id": "ca9c9eee",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Loading ../model/yolov8n.neuron for Neuron (NeuronCore-v1) inference...\n",
                        "\n",
                        "image 1/1 /home/ubuntu/lab/03-yolo8-inf1/notebook/../test_image/bus.jpg: 640x640 4 persons, 1 bus, 29.4ms\n",
                        "Speed: 5.7ms preprocess, 29.4ms inference, 74.9ms postprocess per image at shape (1, 3, 640, 640)\n",
                        "Results saved to \u001b[1mresult_image/predict\u001b[0m\n",
                        "1 label saved to result_image/predict/labels\n"
                    ]
                }
            ],
            "source": [
                "results = m_inf.predict(\"../test_image/bus.jpg\", \n",
                "                            # show=True,\n",
                "                            save=True, \n",
                "                            save_txt=True, \n",
                "                            save_crop=True, \n",
                "                            save_conf=True,\n",
                "                            project='result_image')\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "f68ba52b",
            "metadata": {},
            "source": [
                "### Bounding Box information\n",
                "Refer to the link \n",
                "- [Model Prediction with Ultralytics YOLO](https://docs.ultralytics.com/modes/predict/#working-with-results)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "181884e5",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "ultralytics.engine.results.Boxes object with attributes:\n",
                        "\n",
                        "cls: tensor([0., 0., 0., 5., 0.])\n",
                        "conf: tensor([0.8909, 0.8833, 0.8779, 0.8442, 0.4408])\n",
                        "data: tensor([[6.7083e+02, 3.8008e+02, 8.0986e+02, 8.7969e+02, 8.9086e-01, 0.0000e+00],\n",
                        "        [2.2162e+02, 4.0706e+02, 3.4353e+02, 8.5626e+02, 8.8332e-01, 0.0000e+00],\n",
                        "        [5.0671e+01, 3.9760e+02, 2.4420e+02, 9.0507e+02, 8.7790e-01, 0.0000e+00],\n",
                        "        [3.1541e+01, 2.3063e+02, 8.0153e+02, 7.7584e+02, 8.4424e-01, 5.0000e+00],\n",
                        "        [4.2298e-01, 5.4981e+02, 5.7900e+01, 8.6834e+02, 4.4076e-01, 0.0000e+00]])\n",
                        "id: None\n",
                        "is_track: False\n",
                        "orig_shape: (1080, 810)\n",
                        "shape: torch.Size([5, 6])\n",
                        "xywh: tensor([[740.3431, 629.8870, 139.0354, 499.6159],\n",
                        "        [282.5750, 631.6615, 121.9024, 449.1991],\n",
                        "        [147.4372, 651.3355, 193.5327, 507.4696],\n",
                        "        [416.5346, 503.2327, 769.9878, 545.2150],\n",
                        "        [ 29.1616, 709.0754,  57.4772, 318.5244]])\n",
                        "xywhn: tensor([[0.9140, 0.5832, 0.1716, 0.4626],\n",
                        "        [0.3489, 0.5849, 0.1505, 0.4159],\n",
                        "        [0.1820, 0.6031, 0.2389, 0.4699],\n",
                        "        [0.5142, 0.4660, 0.9506, 0.5048],\n",
                        "        [0.0360, 0.6566, 0.0710, 0.2949]])\n",
                        "xyxy: tensor([[6.7083e+02, 3.8008e+02, 8.0986e+02, 8.7969e+02],\n",
                        "        [2.2162e+02, 4.0706e+02, 3.4353e+02, 8.5626e+02],\n",
                        "        [5.0671e+01, 3.9760e+02, 2.4420e+02, 9.0507e+02],\n",
                        "        [3.1541e+01, 2.3063e+02, 8.0153e+02, 7.7584e+02],\n",
                        "        [4.2298e-01, 5.4981e+02, 5.7900e+01, 8.6834e+02]])\n",
                        "xyxyn: tensor([[8.2818e-01, 3.5193e-01, 9.9983e-01, 8.1453e-01],\n",
                        "        [2.7361e-01, 3.7691e-01, 4.2411e-01, 7.9283e-01],\n",
                        "        [6.2557e-02, 3.6815e-01, 3.0149e-01, 8.3803e-01],\n",
                        "        [3.8939e-02, 2.1354e-01, 9.8954e-01, 7.1837e-01],\n",
                        "        [5.2220e-04, 5.0909e-01, 7.1482e-02, 8.0402e-01]])\n"
                    ]
                }
            ],
            "source": [
                "# View results\n",
                "for r in results:\n",
                "    print(r.boxes)  # print the Boxes object containing the detection bounding boxes"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "ff9324bb",
            "metadata": {},
            "source": [
                "### Benchmarking"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "id": "680ba9e5",
            "metadata": {},
            "outputs": [],
            "source": [
                "# # 이미지 경로\n",
                "# image_path = \"../test_image/bus.jpg\"\n",
                "\n",
                "# # 벤치마킹 실행\n",
                "# results = benchmark_inference(m_inf, image_path, \n",
                "#                               num_runs=50, num_warmup=10)\n",
                "\n",
                "# # 결과 출력\n",
                "# print(f\"Average Inference Time: {results['average_time']:.2f} ms\")\n",
                "# print(f\"Standard Deviation: {results['std_dev']:.2f} ms\")\n",
                "# print(f\"Min Inference Time: {results['min_time']:.2f} ms\")\n",
                "# print(f\"Max Inference Time: {results['max_time']:.2f} ms\")\n",
                "\n",
                "# # 히스토그램 그리기 (선택사항)\n",
                "# import matplotlib.pyplot as plt\n",
                "\n",
                "# plt.hist(results['all_times'], bins=20)\n",
                "# plt.title('Inference Time Distribution')\n",
                "# plt.xlabel('Time (ms)')\n",
                "# plt.ylabel('Frequency')\n",
                "# plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "id": "00fbbc97",
            "metadata": {},
            "outputs": [],
            "source": [
                "# print(\"result_inf2): \\n\", result)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "d824a28c",
            "metadata": {},
            "outputs": [],
            "source": []
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "yolo8-conda-py310",
            "language": "python",
            "name": "yolo8-conda-py310"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.15"
        },
        "vscode": {
            "interpreter": {
                "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
            }
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
