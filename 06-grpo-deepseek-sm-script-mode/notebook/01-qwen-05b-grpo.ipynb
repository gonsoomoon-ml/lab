{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Qwen 0.5b on GRPO\n",
    "\n",
    "---\n",
    "이 노트북은 [will brown](https://x.com/willccbb)이 작성한 [GRPO 데모](https://gist.github.com/willccbb/4676755236bb08cab5f4e54a0475d6fb)의 변형 버전으로, gsm8k 수학 데이터셋을 사용하여 llama-1b를 학습시키는 내용입니다.\n",
    "\n",
    "우리는 Colab에서 더 잘 작동하도록 다음과 같은 변경사항들을 구현했습니다:\n",
    "* llama-1b를 Qwen-0.5b로 교체\n",
    "* vllm을 사용한 생성 방식 도입(상당한 속도 향상). Qwen의 작은 크기 덕분에 GRPO에 사용되는 것과 동일한 GPU에서 vllm을 실행할 수 있습니다\n",
    "* flash-attn 제거 (Qwen 모델링에서 반복적인 버그 발생, 원인 불명확)\n",
    "\n",
    "---\n",
    "Ref:\n",
    "- [Original Notebook](https://colab.research.google.com/drive/1bfhs1FMLW3FGa8ydvkOZyBNxLYOu0Hev?usp=sharing#scrollTo=Q7qTZbUcg5VD)\n",
    "- [Run your local code as a SageMaker training job](https://docs.aws.amazon.com/sagemaker/latest/dg/train-remote-decorator.html)\n",
    "    - [Quick Start - Run local code as SageMaker training job](https://github.com/aws/amazon-sagemaker-examples/blob/main/sagemaker-remote-function/quick_start/quick_start.ipynb)\n",
    "    - [Train a Pre-trained Huggingface Model](https://github.com/aws/amazon-sagemaker-examples/blob/main/sagemaker-remote-function/huggingface_text_classification/huggingface.ipynb)\n",
    "    - \n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 환경 설정\n",
    "- 시작을 위해서 여기 가이드 보세요: [Setup Guide](../setup/README.md)\n",
    "- 이후에 아래 셀 실행을 통해서, 필요한 패키지가 설치 되었는지를 확인 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datasets                           3.2.0\n",
      "sagemaker                          2.232.2\n",
      "sagemaker-core                     1.0.21\n",
      "sagemaker-mlflow                   0.1.0\n",
      "trl                                0.14.0\n",
      "vllm                               0.7.2\n"
     ]
    }
   ],
   "source": [
    "! pip list | grep -E \"sagemaker|vllm|trl|datasets\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## System Prompt , COT Format 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sagemaker-user/.conda/envs/grpo/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 02-09 04:17:27 __init__.py:190] Automatically detected platform cuda.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-09 04:17:27,828\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import torch\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from trl import GRPOConfig, GRPOTrainer\n",
    "\n",
    "# Load and prep dataset\n",
    "\n",
    "SYSTEM_PROMPT = \"\"\"\n",
    "Respond in the following format:\n",
    "<reasoning>\n",
    "...\n",
    "</reasoning>\n",
    "<answer>\n",
    "...\n",
    "</answer>\n",
    "\"\"\"\n",
    "\n",
    "XML_COT_FORMAT = \"\"\"\\\n",
    "<reasoning>\n",
    "{reasoning}\n",
    "</reasoning>\n",
    "<answer>\n",
    "{answer}\n",
    "</answer>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터 셋 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_xml_answer(text: str) -> str:\n",
    "    answer = text.split(\"<answer>\")[-1]\n",
    "    answer = answer.split(\"</answer>\")[0]\n",
    "    return answer.strip()\n",
    "\n",
    "def extract_hash_answer(text: str) -> str | None:\n",
    "    if \"####\" not in text:\n",
    "        return None\n",
    "    return text.split(\"####\")[1].strip()\n",
    "\n",
    "# uncomment middle messages for 1-shot prompting\n",
    "def get_gsm8k_questions(split = \"train\") -> Dataset:\n",
    "    data = load_dataset('openai/gsm8k', 'main')[split] # type: ignore\n",
    "    data = data.map(lambda x: { # type: ignore\n",
    "        'prompt': [\n",
    "            {'role': 'system', 'content': SYSTEM_PROMPT},\n",
    "            {'role': 'user', 'content': x['question']}\n",
    "        ],\n",
    "        'answer': extract_hash_answer(x['answer'])\n",
    "    }) # type: ignore\n",
    "    return data # type: ignore\n",
    "\n",
    "dataset = get_gsm8k_questions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 리워드 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reward functions\n",
    "def correctness_reward_func(prompts, completions, answer, **kwargs) -> list[float]:\n",
    "    responses = [completion[0]['content'] for completion in completions]\n",
    "    q = prompts[0][-1]['content']\n",
    "    extracted_responses = [extract_xml_answer(r) for r in responses]\n",
    "    print('-'*20, f\"Question:\\n{q}\", f\"\\nAnswer:\\n{answer[0]}\", f\"\\nResponse:\\n{responses[0]}\", f\"\\nExtracted:\\n{extracted_responses[0]}\")\n",
    "    return [2.0 if r == a else 0.0 for r, a in zip(extracted_responses, answer)]\n",
    "\n",
    "def int_reward_func(completions, **kwargs) -> list[float]:\n",
    "    responses = [completion[0]['content'] for completion in completions]\n",
    "    extracted_responses = [extract_xml_answer(r) for r in responses]\n",
    "    return [0.5 if r.isdigit() else 0.0 for r in extracted_responses]\n",
    "\n",
    "def strict_format_reward_func(completions, **kwargs) -> list[float]:\n",
    "    \"\"\"Reward function that checks if the completion has a specific format.\"\"\"\n",
    "    pattern = r\"^<reasoning>\\n.*?\\n</reasoning>\\n<answer>\\n.*?\\n</answer>\\n$\"\n",
    "    responses = [completion[0][\"content\"] for completion in completions]\n",
    "    matches = [re.match(pattern, r) for r in responses]\n",
    "    return [0.5 if match else 0.0 for match in matches]\n",
    "\n",
    "def soft_format_reward_func(completions, **kwargs) -> list[float]:\n",
    "    \"\"\"Reward function that checks if the completion has a specific format.\"\"\"\n",
    "    pattern = r\"<reasoning>.*?</reasoning>\\s*<answer>.*?</answer>\"\n",
    "    responses = [completion[0][\"content\"] for completion in completions]\n",
    "    matches = [re.match(pattern, r) for r in responses]\n",
    "    return [0.5 if match else 0.0 for match in matches]\n",
    "\n",
    "def count_xml(text) -> float:\n",
    "    count = 0.0\n",
    "    if text.count(\"<reasoning>\\n\") == 1:\n",
    "        count += 0.125\n",
    "    if text.count(\"\\n</reasoning>\\n\") == 1:\n",
    "        count += 0.125\n",
    "    if text.count(\"\\n<answer>\\n\") == 1:\n",
    "        count += 0.125\n",
    "        count -= len(text.split(\"\\n</answer>\\n\")[-1])*0.001\n",
    "    if text.count(\"\\n</answer>\") == 1:\n",
    "        count += 0.125\n",
    "        count -= (len(text.split(\"\\n</answer>\")[-1]) - 1)*0.001\n",
    "    return count\n",
    "\n",
    "def xmlcount_reward_func(completions, **kwargs) -> list[float]:\n",
    "    contents = [completion[0][\"content\"] for completion in completions]\n",
    "    return [count_xml(c) for c in contents]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 훈련 파라미터 정의\n",
    "- 모델 정의\n",
    "- 훈련 파라미터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"Qwen/Qwen2.5-0.5B-Instruct\"\n",
    "\n",
    "output_dir=\"outputs/Qwen-0.5B-GRPO\"\n",
    "run_name=\"Qwen-0.5B-GRPO-gsm8k\"\n",
    "\n",
    "training_args = GRPOConfig(\n",
    "    output_dir=output_dir,\n",
    "    run_name=run_name,\n",
    "    learning_rate=5e-6,\n",
    "    adam_beta1 = 0.9,\n",
    "    adam_beta2 = 0.99,\n",
    "    weight_decay = 0.1,\n",
    "    warmup_ratio = 0.1,\n",
    "    lr_scheduler_type='cosine',\n",
    "    logging_steps=1,\n",
    "    bf16=True,\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=4,\n",
    "    num_generations=16,\n",
    "    max_prompt_length=256,\n",
    "    max_completion_length=200,\n",
    "    num_train_epochs=1,\n",
    "    save_steps=100,\n",
    "    max_grad_norm=0.1,\n",
    "    log_on_each_node=False,\n",
    "    use_vllm=True,\n",
    "    vllm_gpu_memory_utilization=.3,\n",
    "    vllm_device=\"cuda:0\",\n",
    "    report_to=\"none\" #I'm disabling Wandb.\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델 로딩, 토큰나이저 로딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=None\n",
    ").to(\"cuda\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## S3 Root 폴더 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/sagemaker-user/.config/sagemaker/config.yaml\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "\n",
    "sm_session = sagemaker.Session()\n",
    "\n",
    "s3_root_folder = f\"s3://{sm_session.default_bucket()}/deepseek/qrpo\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 리모트르로 SageMaker Training Job 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.telemetry.telemetry_logging:SageMaker Python SDK will collect telemetry to help us better understand our user's needs, diagnose issues, and deliver additional features.\n",
      "To opt out of telemetry, please disable via TelemetryOptOut parameter in SDK defaults config. For more information, refer to https://sagemaker.readthedocs.io/en/stable/overview.html#configuring-and-using-defaults-with-the-sagemaker-python-sdk.\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.remote_function import remote\n",
    "\n",
    "@remote(instance_type=\"ml.p4d.24xlarge\", dependencies='./requirements.txt', keep_alive_period_in_seconds=3600)\n",
    "def wrap_hf_trainer(model, tokenizer, training_args, xmlcount_reward_func, dataset, soft_format_reward_func,int_reward_func, correctness_reward_func):\n",
    "    trainer = GRPOTrainer(\n",
    "        model=model,\n",
    "        processing_class=tokenizer,\n",
    "        reward_funcs=[\n",
    "            xmlcount_reward_func,\n",
    "            soft_format_reward_func,\n",
    "            strict_format_reward_func,\n",
    "            int_reward_func,\n",
    "            correctness_reward_func],\n",
    "        args=training_args,\n",
    "        train_dataset=dataset,\n",
    "        #peft_config=peft_config\n",
    "    )\n",
    "    trainer.train()\n",
    "\n",
    "    return trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-09 04:17:32,719 sagemaker.remote_function INFO     Serializing function code to s3://sagemaker-us-east-1-057716757052/wrap-hf-trainer-2025-02-09-04-17-32-719/function\n",
      "2025-02-09 04:17:32,777 sagemaker.remote_function INFO     Serializing function arguments to s3://sagemaker-us-east-1-057716757052/wrap-hf-trainer-2025-02-09-04-17-32-719/arguments\n",
      "2025-02-09 04:17:41,493 sagemaker.remote_function INFO     Copied dependencies file at './requirements.txt' to '/tmp/tmpqbz9trxe/temp_workspace/sagemaker_remote_function_workspace/requirements.txt'\n",
      "2025-02-09 04:17:41,495 sagemaker.remote_function INFO     Successfully created workdir archive at '/tmp/tmpqbz9trxe/workspace.zip'\n",
      "2025-02-09 04:17:41,554 sagemaker.remote_function INFO     Successfully uploaded workdir to 's3://sagemaker-us-east-1-057716757052/wrap-hf-trainer-2025-02-09-04-17-32-719/sm_rf_user_ws/workspace.zip'\n",
      "2025-02-09 04:17:41,555 sagemaker.remote_function INFO     Creating job: wrap-hf-trainer-2025-02-09-04-17-32-719\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-02-09 04:17:42 Starting - Starting the training job\n",
      "2025-02-09 04:17:42 Pending - Training job waiting for capacity................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/sagemaker-user/lab/05-grpo-deepseek/notebook/01-qwen-05b-grpo.ipynb Cell 18\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://3vpfglwxr3vjrns.studio.us-east-1.sagemaker.aws/home/sagemaker-user/lab/05-grpo-deepseek/notebook/01-qwen-05b-grpo.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m wrap_hf_trainer(model, tokenizer, training_args, xmlcount_reward_func, dataset, soft_format_reward_func,int_reward_func, correctness_reward_func)\n",
      "File \u001b[0;32m~/.conda/envs/grpo/lib/python3.10/site-packages/sagemaker/remote_function/client.py:339\u001b[0m, in \u001b[0;36mremote.<locals>._remote.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    336\u001b[0m job \u001b[39m=\u001b[39m _Job\u001b[39m.\u001b[39mstart(job_settings, func, args, kwargs)\n\u001b[1;32m    338\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 339\u001b[0m     job\u001b[39m.\u001b[39;49mwait()\n\u001b[1;32m    340\u001b[0m \u001b[39mexcept\u001b[39;00m UnexpectedStatusException \u001b[39mas\u001b[39;00m usex:\n\u001b[1;32m    341\u001b[0m     \u001b[39mif\u001b[39;00m usex\u001b[39m.\u001b[39mactual_status \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mFailed\u001b[39m\u001b[39m\"\u001b[39m:\n",
      "File \u001b[0;32m~/.conda/envs/grpo/lib/python3.10/site-packages/sagemaker/remote_function/job.py:973\u001b[0m, in \u001b[0;36m_Job.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    960\u001b[0m \u001b[39mdef\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mwait\u001b[39m(\u001b[39mself\u001b[39m, timeout: \u001b[39mint\u001b[39m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m):\n\u001b[1;32m    961\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Wait for the underlying sagemaker job to finish and displays its logs .\u001b[39;00m\n\u001b[1;32m    962\u001b[0m \n\u001b[1;32m    963\u001b[0m \u001b[39m    This method blocks on the sagemaker job completing for up to the timeout value (if\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    970\u001b[0m \u001b[39m    Returns: None\u001b[39;00m\n\u001b[1;32m    971\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 973\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_last_describe_response \u001b[39m=\u001b[39m _logs_for_job(\n\u001b[1;32m    974\u001b[0m         sagemaker_session\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msagemaker_session,\n\u001b[1;32m    975\u001b[0m         job_name\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mjob_name,\n\u001b[1;32m    976\u001b[0m         wait\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    977\u001b[0m         timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[1;32m    978\u001b[0m     )\n",
      "File \u001b[0;32m~/.conda/envs/grpo/lib/python3.10/site-packages/sagemaker/session.py:8497\u001b[0m, in \u001b[0;36m_logs_for_job\u001b[0;34m(sagemaker_session, job_name, wait, poll, log_type, timeout)\u001b[0m\n\u001b[1;32m   8494\u001b[0m \u001b[39mif\u001b[39;00m state \u001b[39m==\u001b[39m LogState\u001b[39m.\u001b[39mCOMPLETE:\n\u001b[1;32m   8495\u001b[0m     \u001b[39mbreak\u001b[39;00m\n\u001b[0;32m-> 8497\u001b[0m time\u001b[39m.\u001b[39;49msleep(poll)\n\u001b[1;32m   8499\u001b[0m \u001b[39mif\u001b[39;00m state \u001b[39m==\u001b[39m LogState\u001b[39m.\u001b[39mJOB_COMPLETE:\n\u001b[1;32m   8500\u001b[0m     state \u001b[39m=\u001b[39m LogState\u001b[39m.\u001b[39mCOMPLETE\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "wrap_hf_trainer(model, tokenizer, training_args, xmlcount_reward_func, dataset, soft_format_reward_func,int_reward_func, correctness_reward_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
