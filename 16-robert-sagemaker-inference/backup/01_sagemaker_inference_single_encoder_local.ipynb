{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SageMaker Inference from Scratch\n",
    "\n",
    "KLUE RoBERTa ëª¨ë¸ì„ SageMaker Endpointë¡œ ë°°í¬í•˜ê³  ì¶”ë¡ í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. ë¡œì»¬ ì¶”ë¡  í•¨ìˆ˜ í…ŒìŠ¤íŠ¸\n",
    "\n",
    "ë¨¼ì € ë¡œì»¬ì—ì„œ inference.pyê°€ ì •ìƒ ë™ì‘í•˜ëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/lab/16-robert-sagemaker-inference/.venv/bin/python\n",
      "\u001b[2mUsing Python 3.11.0rc1 environment at: /home/ubuntu/lab/16-robert-sagemaker-inference/.venv\u001b[0m\n",
      "torch                            2.5.0\n"
     ]
    }
   ],
   "source": [
    "! which python\n",
    "! uv pip list | grep torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Testing SageMaker Inference Functions\n",
      "============================================================\n",
      "\n",
      "1. Testing model_fn...\n",
      "Loading model from .\n",
      "Using device: cuda\n",
      "Downloading model from HuggingFace: klue/roberta-base\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at klue/roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Model loaded successfully\n",
      "   âœ“ Model loaded\n",
      "\n",
      "2. Testing input_fn...\n",
      "Received content_type: application/json\n",
      "   âœ“ Processed 3 texts\n",
      "\n",
      "3. Testing predict_fn...\n",
      "Starting prediction\n",
      "Processing 3 text(s)\n",
      "Prediction completed successfully\n",
      "   âœ“ Embedding shape: (3, 768)\n",
      "\n",
      "4. Testing output_fn...\n",
      "Formatting output with accept: application/json\n",
      "   âœ“ Output keys: ['embeddings', 'embedding_dim', 'num_texts']\n",
      "\n",
      "============================================================\n",
      "All tests passed!\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "!python ../src/test_inference.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. í™˜ê²½ ì„¤ì •"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/ubuntu/.config/sagemaker/config.yaml\n",
      "Bucket: sagemaker-us-east-1-057716757052\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import time\n",
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker.pytorch.model import PyTorchModel\n",
    "from sagemaker.serializers import JSONSerializer\n",
    "from sagemaker.deserializers import JSONDeserializer\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "role = \"arn:aws:iam::057716757052:role/gonsoomoon-sm-inference\"\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "\n",
    "print(f\"Bucket: {bucket}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. ëª¨ë¸ ì•„í‹°íŒ©íŠ¸ ìƒì„± ë° S3 ì—…ë¡œë“œ\n",
    "\n",
    "model.tar.gz êµ¬ì¡°ë¡œ ìƒì„±ì„ í•˜ë©´ , SageMaker ê°€ ì´ë¥¼ ì¸ì§€ í•©ë‹ˆë‹¤.\n",
    "model.tar.gz êµ¬ì¡°:\n",
    "```\n",
    "model.tar.gz/\n",
    "â”œâ”€â”€ config.json\n",
    "â”œâ”€â”€ model.safetensors\n",
    "â”œâ”€â”€ special_tokens_map.json\n",
    "â”œâ”€â”€ tokenizer.json\n",
    "â”œâ”€â”€ tokenizer_config.json\n",
    "â”œâ”€â”€ vocab.txt\n",
    "â””â”€â”€ code/\n",
    "    â”œâ”€â”€ inference.py\n",
    "    â””â”€â”€ requirements.txt\n",
    "```\n",
    "\n",
    "ì°¸ì¡°: [SageMaker PyTorch Documentation](https://sagemaker.readthedocs.io/en/stable/frameworks/pytorch/using_pytorch.html#deploy-pytorch-models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model.tar.gz íŒŒì¼ ìƒì„±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "upload: ../model.tar.gz to s3://sagemaker-us-east-1-057716757052/klue-roberta-inference/model/model.tar.gz\n",
      "Model uploaded to: s3://sagemaker-us-east-1-057716757052/klue-roberta-inference/model/model.tar.gz\n"
     ]
    }
   ],
   "source": [
    "!rm -rf ../model_artifact\n",
    "!mkdir -p ../model_artifact/code\n",
    "!cp ../src/inference.py ../model_artifact/code/\n",
    "!cp ../src/requirements.txt ../model_artifact/code/\n",
    "!cp ../model/* ../model_artifact/\n",
    "\n",
    "!cd ../model_artifact && tar -czf ../model.tar.gz *\n",
    "\n",
    "model_artifact_s3_uri = f's3://{bucket}/klue-roberta-inference/model/model.tar.gz'\n",
    "!aws s3 cp ../model.tar.gz {model_artifact_s3_uri}\n",
    "\n",
    "print(f\"Model uploaded to: {model_artifact_s3_uri}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. SageMaker Endpoint ìƒì„±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "local_model_path:  ../model.tar.gz\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "model_data_dir = \"..\"\n",
    "local_model_path = os.path.join(model_data_dir, 'model.tar.gz')\n",
    "print(\"local_model_path: \", local_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Sep 27 04:29:45 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 570.133.20             Driver Version: 570.133.20     CUDA Version: 12.8     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA L4                      On  |   00000000:36:00.0 Off |                    0 |\n",
      "| N/A   42C    P8             16W /   72W |       0MiB /  23034MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instance type = local_gpu\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import subprocess\n",
    "\n",
    "try:\n",
    "    if subprocess.call(\"nvidia-smi\") == 0:\n",
    "        ## Set type to GPU if one is present\n",
    "        instance_type = \"local_gpu\"\n",
    "    else:\n",
    "        instance_type = \"local\"        \n",
    "except:\n",
    "    pass\n",
    "\n",
    "print(\"Instance type = \" + instance_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RootlessDocker not detected, falling back to remote host IP or localhost.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attaching to f3li42g0j0-algo-1-0f1av\n",
      "f3li42g0j0-algo-1-0f1av  | CUDA compat package should be installed for NVIDIA driver smaller than 550.163.01\n",
      "f3li42g0j0-algo-1-0f1av  | Current installed NVIDIA driver version is 570.133.20\n",
      "f3li42g0j0-algo-1-0f1av  | Skipping CUDA compat setup as newer NVIDIA driver is installed\n",
      "f3li42g0j0-algo-1-0f1av  | /opt/conda/lib/python3.11/site-packages/sagemaker_pytorch_serving_container/torchserve.py:20: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "f3li42g0j0-algo-1-0f1av  |   import pkg_resources\n",
      "f3li42g0j0-algo-1-0f1av  | Collecting transformers>=4.30.0 (from -r /opt/ml/model/code/requirements.txt (line 1))\n",
      "f3li42g0j0-algo-1-0f1av  |   Downloading transformers-4.56.2-py3-none-any.whl.metadata (40 kB)\n",
      "f3li42g0j0-algo-1-0f1av  | Requirement already satisfied: filelock in /opt/conda/lib/python3.11/site-packages (from transformers>=4.30.0->-r /opt/ml/model/code/requirements.txt (line 1)) (3.18.0)\n",
      "f3li42g0j0-algo-1-0f1av  | Collecting huggingface-hub<1.0,>=0.34.0 (from transformers>=4.30.0->-r /opt/ml/model/code/requirements.txt (line 1))\n",
      "f3li42g0j0-algo-1-0f1av  |   Downloading huggingface_hub-0.35.1-py3-none-any.whl.metadata (14 kB)\n",
      "f3li42g0j0-algo-1-0f1av  | Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.11/site-packages (from transformers>=4.30.0->-r /opt/ml/model/code/requirements.txt (line 1)) (2.2.6)\n",
      "f3li42g0j0-algo-1-0f1av  | Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.11/site-packages (from transformers>=4.30.0->-r /opt/ml/model/code/requirements.txt (line 1)) (23.2)\n",
      "f3li42g0j0-algo-1-0f1av  | Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.11/site-packages (from transformers>=4.30.0->-r /opt/ml/model/code/requirements.txt (line 1)) (6.0.1)\n",
      "f3li42g0j0-algo-1-0f1av  | Collecting regex!=2019.12.17 (from transformers>=4.30.0->-r /opt/ml/model/code/requirements.txt (line 1))\n",
      "f3li42g0j0-algo-1-0f1av  |   Downloading regex-2025.9.18-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (40 kB)\n",
      "f3li42g0j0-algo-1-0f1av  | Requirement already satisfied: requests in /opt/conda/lib/python3.11/site-packages (from transformers>=4.30.0->-r /opt/ml/model/code/requirements.txt (line 1)) (2.32.4)\n",
      "f3li42g0j0-algo-1-0f1av  | Collecting tokenizers<=0.23.0,>=0.22.0 (from transformers>=4.30.0->-r /opt/ml/model/code/requirements.txt (line 1))\n",
      "f3li42g0j0-algo-1-0f1av  |   Downloading tokenizers-0.22.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "f3li42g0j0-algo-1-0f1av  | Collecting safetensors>=0.4.3 (from transformers>=4.30.0->-r /opt/ml/model/code/requirements.txt (line 1))\n",
      "f3li42g0j0-algo-1-0f1av  |   Downloading safetensors-0.6.2-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
      "f3li42g0j0-algo-1-0f1av  | Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.11/site-packages (from transformers>=4.30.0->-r /opt/ml/model/code/requirements.txt (line 1)) (4.66.5)\n",
      "f3li42g0j0-algo-1-0f1av  | Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers>=4.30.0->-r /opt/ml/model/code/requirements.txt (line 1)) (2025.7.0)\n",
      "f3li42g0j0-algo-1-0f1av  | Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers>=4.30.0->-r /opt/ml/model/code/requirements.txt (line 1)) (4.14.1)\n",
      "f3li42g0j0-algo-1-0f1av  | Collecting hf-xet<2.0.0,>=1.1.3 (from huggingface-hub<1.0,>=0.34.0->transformers>=4.30.0->-r /opt/ml/model/code/requirements.txt (line 1))\n",
      "f3li42g0j0-algo-1-0f1av  |   Downloading hf_xet-1.1.10-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.7 kB)\n",
      "f3li42g0j0-algo-1-0f1av  | Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests->transformers>=4.30.0->-r /opt/ml/model/code/requirements.txt (line 1)) (3.3.2)\n",
      "f3li42g0j0-algo-1-0f1av  | Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests->transformers>=4.30.0->-r /opt/ml/model/code/requirements.txt (line 1)) (3.10)\n",
      "f3li42g0j0-algo-1-0f1av  | Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests->transformers>=4.30.0->-r /opt/ml/model/code/requirements.txt (line 1)) (2.5.0)\n",
      "f3li42g0j0-algo-1-0f1av  | Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests->transformers>=4.30.0->-r /opt/ml/model/code/requirements.txt (line 1)) (2025.7.14)\n",
      "f3li42g0j0-algo-1-0f1av  | Downloading transformers-4.56.2-py3-none-any.whl (11.6 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m11.6/11.6 MB\u001b[0m \u001b[31m146.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m-:--:--\u001b[0m\n",
      "f3li42g0j0-algo-1-0f1av  | \u001b[?25hDownloading huggingface_hub-0.35.1-py3-none-any.whl (563 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m563.3/563.3 kB\u001b[0m \u001b[31m36.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m-:--:--\u001b[0m\n",
      "f3li42g0j0-algo-1-0f1av  | \u001b[?25hDownloading hf_xet-1.1.10-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.2 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m138.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0meta \u001b[36m-:--:--\u001b[0m\n",
      "f3li42g0j0-algo-1-0f1av  | \u001b[?25hDownloading tokenizers-0.22.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m145.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0meta \u001b[36m-:--:--\u001b[0m\n",
      "f3li42g0j0-algo-1-0f1av  | \u001b[?25hDownloading regex-2025.9.18-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (798 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m799.0/799.0 kB\u001b[0m \u001b[31m54.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m-:--:--\u001b[0m\n",
      "f3li42g0j0-algo-1-0f1av  | \u001b[?25hDownloading safetensors-0.6.2-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (485 kB)\n",
      "f3li42g0j0-algo-1-0f1av  | Installing collected packages: safetensors, regex, hf-xet, huggingface-hub, tokenizers, transformers\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m6/6\u001b[0m [transformers][0m [transformers]ub]\n",
      "\u001b[1A\u001b[2KSuccessfully installed hf-xet-1.1.10 huggingface-hub-0.35.1 regex-2025.9.18 safetensors-0.6.2 tokenizers-0.22.1 transformers-4.56.2\n",
      "f3li42g0j0-algo-1-0f1av  | \u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "f3li42g0j0-algo-1-0f1av  | \u001b[0m\n",
      "f3li42g0j0-algo-1-0f1av  | \u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "f3li42g0j0-algo-1-0f1av  | \u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "f3li42g0j0-algo-1-0f1av  | ['torchserve', '--start', '--model-store', '/.sagemaker/ts/models', '--ts-config', '/etc/sagemaker-ts.properties', '--log-config', '/opt/conda/lib/python3.11/site-packages/sagemaker_pytorch_serving_container/etc/log4j2.xml', '--models', 'model=/opt/ml/model']\n",
      "f3li42g0j0-algo-1-0f1av  | Warning: TorchServe is using non-default JVM parameters: -XX:-UseContainerSupport\n",
      "f3li42g0j0-algo-1-0f1av  | WARNING: sun.reflect.Reflection.getCallerClass is not supported. This will impact performance.\n",
      "f3li42g0j0-algo-1-0f1av  | 2025-09-27T04:32:17,263 [WARN ] main org.pytorch.serve.util.ConfigManager - Your torchserve instance can access any URL to load models. When deploying to production, make sure to limit the set of allowed_urls in config.properties\n",
      "f3li42g0j0-algo-1-0f1av  | 2025-09-27T04:32:17,272 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...\n",
      "f3li42g0j0-algo-1-0f1av  | 2025-09-27T04:32:17,318 [INFO ] main org.pytorch.serve.metrics.configuration.MetricConfiguration - Successfully loaded metrics configuration from /opt/conda/lib/python3.11/site-packages/ts/configs/metrics.yaml\n",
      "f3li42g0j0-algo-1-0f1av  | 2025-09-27T04:32:17,394 [INFO ] main org.pytorch.serve.ModelServer - \n",
      "f3li42g0j0-algo-1-0f1av  | Torchserve version: 0.12.0\n",
      "f3li42g0j0-algo-1-0f1av  | TS Home: /opt/conda/lib/python3.11/site-packages\n",
      "f3li42g0j0-algo-1-0f1av  | Current directory: /\n",
      "f3li42g0j0-algo-1-0f1av  | Temp directory: /tmp\n",
      "f3li42g0j0-algo-1-0f1av  | Metrics config path: /opt/conda/lib/python3.11/site-packages/ts/configs/metrics.yaml\n",
      "f3li42g0j0-algo-1-0f1av  | Number of GPUs: 1\n",
      "f3li42g0j0-algo-1-0f1av  | Number of CPUs: 32\n",
      "f3li42g0j0-algo-1-0f1av  | Max heap size: 30688 M\n",
      "f3li42g0j0-algo-1-0f1av  | Python executable: /opt/conda/bin/python3.11\n",
      "f3li42g0j0-algo-1-0f1av  | Config file: /etc/sagemaker-ts.properties\n",
      "f3li42g0j0-algo-1-0f1av  | Inference address: http://0.0.0.0:8080\n",
      "f3li42g0j0-algo-1-0f1av  | Management address: http://0.0.0.0:8080\n",
      "f3li42g0j0-algo-1-0f1av  | Metrics address: http://127.0.0.1:8082\n",
      "f3li42g0j0-algo-1-0f1av  | Model Store: /.sagemaker/ts/models\n",
      "f3li42g0j0-algo-1-0f1av  | Initial Models: model=/opt/ml/model\n",
      "f3li42g0j0-algo-1-0f1av  | Log dir: /logs\n",
      "f3li42g0j0-algo-1-0f1av  | Metrics dir: /logs\n",
      "f3li42g0j0-algo-1-0f1av  | Netty threads: 0\n",
      "f3li42g0j0-algo-1-0f1av  | Netty client threads: 0\n",
      "f3li42g0j0-algo-1-0f1av  | Default workers per model: 1\n",
      "f3li42g0j0-algo-1-0f1av  | Blacklist Regex: N/A\n",
      "f3li42g0j0-algo-1-0f1av  | Maximum Response Size: 6553500\n",
      "f3li42g0j0-algo-1-0f1av  | Maximum Request Size: 6553500\n",
      "f3li42g0j0-algo-1-0f1av  | Limit Maximum Image Pixels: true\n",
      "f3li42g0j0-algo-1-0f1av  | Prefer direct buffer: false\n",
      "f3li42g0j0-algo-1-0f1av  | Allowed Urls: [file://.*|http(s)?://.*]\n",
      "f3li42g0j0-algo-1-0f1av  | Custom python dependency for model allowed: false\n",
      "f3li42g0j0-algo-1-0f1av  | Enable metrics API: true\n",
      "f3li42g0j0-algo-1-0f1av  | Metrics mode: LOG\n",
      "f3li42g0j0-algo-1-0f1av  | Disable system metrics: true\n",
      "f3li42g0j0-algo-1-0f1av  | Workflow Store: /.sagemaker/ts/models\n",
      "f3li42g0j0-algo-1-0f1av  | CPP log config: N/A\n",
      "f3li42g0j0-algo-1-0f1av  | Model config: N/A\n",
      "f3li42g0j0-algo-1-0f1av  | System metrics command: default\n",
      "f3li42g0j0-algo-1-0f1av  | Model API enabled: false\n",
      "f3li42g0j0-algo-1-0f1av  | 2025-09-27T04:32:17,401 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...\n",
      "f3li42g0j0-algo-1-0f1av  | 2025-09-27T04:32:17,416 [INFO ] main org.pytorch.serve.ModelServer - Loading initial models: /opt/ml/model\n",
      "f3li42g0j0-algo-1-0f1av  | 2025-09-27T04:32:17,422 [INFO ] main org.pytorch.serve.archive.model.ModelArchive - createTempDir /tmp/models/fb069205ef3e4c69ae53b1ac8fa09463\n",
      "f3li42g0j0-algo-1-0f1av  | 2025-09-27T04:32:17,422 [INFO ] main org.pytorch.serve.archive.model.ModelArchive - createSymbolicDir /tmp/models/fb069205ef3e4c69ae53b1ac8fa09463/model\n",
      "f3li42g0j0-algo-1-0f1av  | 2025-09-27T04:32:17,423 [WARN ] main org.pytorch.serve.archive.model.ModelArchive - Model archive version is not defined. Please upgrade to torch-model-archiver 0.2.0 or higher\n",
      "f3li42g0j0-algo-1-0f1av  | 2025-09-27T04:32:17,423 [WARN ] main org.pytorch.serve.archive.model.ModelArchive - Model archive createdOn is not defined. Please upgrade to torch-model-archiver 0.2.0 or higher\n",
      "f3li42g0j0-algo-1-0f1av  | 2025-09-27T04:32:17,425 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model model loaded.\n",
      "f3li42g0j0-algo-1-0f1av  | 2025-09-27T04:32:17,431 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: EpollServerSocketChannel.\n",
      "f3li42g0j0-algo-1-0f1av  | 2025-09-27T04:32:17,475 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://0.0.0.0:8080\n",
      "f3li42g0j0-algo-1-0f1av  | 2025-09-27T04:32:17,475 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: EpollServerSocketChannel.\n",
      "f3li42g0j0-algo-1-0f1av  | 2025-09-27T04:32:17,476 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://127.0.0.1:8082\n",
      "f3li42g0j0-algo-1-0f1av  | Model server started.\n",
      "f3li42g0j0-algo-1-0f1av  | 2025-09-27T04:32:18,576 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9000, pid=98\n",
      "f3li42g0j0-algo-1-0f1av  | 2025-09-27T04:32:18,577 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000\n",
      "f3li42g0j0-algo-1-0f1av  | 2025-09-27T04:32:18,582 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Successfully loaded /opt/conda/lib/python3.11/site-packages/ts/configs/metrics.yaml.\n",
      "f3li42g0j0-algo-1-0f1av  | 2025-09-27T04:32:18,582 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - [PID]98\n",
      "f3li42g0j0-algo-1-0f1av  | 2025-09-27T04:32:18,582 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Torch worker started.\n",
      "f3li42g0j0-algo-1-0f1av  | 2025-09-27T04:32:18,583 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Python runtime: 3.11.9\n",
      "f3li42g0j0-algo-1-0f1av  | 2025-09-27T04:32:18,586 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000\n",
      "f3li42g0j0-algo-1-0f1av  | 2025-09-27T04:32:18,592 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.\n",
      "f3li42g0j0-algo-1-0f1av  | 2025-09-27T04:32:18,603 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1758947538603\n",
      "f3li42g0j0-algo-1-0f1av  | 2025-09-27T04:32:18,627 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - model_name: model, batchSize: 1\n",
      "f3li42g0j0-algo-1-0f1av  | 2025-09-27T04:32:18,650 [INFO ] pool-2-thread-2 ACCESS_LOG - /172.18.0.1:49830 \"GET /ping HTTP/1.1\" 200 8\n",
      "f3li42g0j0-algo-1-0f1av  | 2025-09-27T04:32:18,651 [INFO ] pool-2-thread-2 TS_METRICS - Requests2XX.Count:1.0|#Level:Host|#hostname:27c38f57aa3a,timestamp:1758947538\n",
      "f3li42g0j0-algo-1-0f1av  | 2025-09-27T04:32:20,182 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Loading model from /tmp/models/fb069205ef3e4c69ae53b1ac8fa09463/model\n",
      "f3li42g0j0-algo-1-0f1av  | 2025-09-27T04:32:20,183 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Loading model from /tmp/models/fb069205ef3e4c69ae53b1ac8fa09463/model\n",
      "f3li42g0j0-algo-1-0f1av  | 2025-09-27T04:32:20,203 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Using device: cuda\n",
      "f3li42g0j0-algo-1-0f1av  | 2025-09-27T04:32:20,204 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Using device: cuda\n",
      "f3li42g0j0-algo-1-0f1av  | 2025-09-27T04:32:20,204 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Loading model from model_dir: /tmp/models/fb069205ef3e4c69ae53b1ac8fa09463/model\n",
      "f3li42g0j0-algo-1-0f1av  | 2025-09-27T04:32:20,204 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Loading model from model_dir: /tmp/models/fb069205ef3e4c69ae53b1ac8fa09463/model\n",
      "f3li42g0j0-algo-1-0f1av  | 2025-09-27T04:32:21,414 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Model loaded successfully\n",
      "f3li42g0j0-algo-1-0f1av  | 2025-09-27T04:32:21,415 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Model loaded successfully\n",
      "f3li42g0j0-algo-1-0f1av  | 2025-09-27T04:32:21,415 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 2812\n",
      "f3li42g0j0-algo-1-0f1av  | 2025-09-27T04:32:21,416 [INFO ] W-9000-model_1.0 TS_METRICS - WorkerLoadTime.Milliseconds:3988.0|#WorkerName:W-9000-model_1.0,Level:Host|#hostname:27c38f57aa3a,timestamp:1758947541\n",
      "f3li42g0j0-algo-1-0f1av  | 2025-09-27T04:32:21,416 [INFO ] W-9000-model_1.0 TS_METRICS - WorkerThreadTime.Milliseconds:3.0|#Level:Host|#hostname:27c38f57aa3a,timestamp:1758947541\n",
      "!"
     ]
    }
   ],
   "source": [
    "endpoint_name = \"local-endpoint-ncf-{}\".format(int(time.time()))\n",
    "\n",
    "local_pytorch_model = PyTorchModel(model_data=local_model_path,\n",
    "                                   role=role,\n",
    "                                   entry_point='inference.py',\n",
    "                                   source_dir = '../src',\n",
    "                                   framework_version='2.5',\n",
    "                                   py_version='py311',\n",
    "                                   model_server_workers=1,\n",
    "                                  )\n",
    "\n",
    "local_predictor = local_pytorch_model.deploy(\n",
    "                           instance_type=instance_type, \n",
    "                           initial_instance_count=1, \n",
    "                           endpoint_name=endpoint_name,\n",
    "                           wait=True,\n",
    "                           log = False,\n",
    "                        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ì´ì „ ì½”ë“œ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Endpoint ì¶”ë¡ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RootlessDocker not detected, falling back to remote host IP or localhost.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f3li42g0j0-algo-1-0f1av  | 2025-09-27T04:32:48,837 [INFO ] epollEventLoopGroup-3-2 TS_METRICS - ts_inference_requests_total.Count:1.0|#model_name:model,model_version:default|#hostname:27c38f57aa3a,timestamp:1758947568\n",
      "f3li42g0j0-algo-1-0f1av  | 2025-09-27T04:32:48,838 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1758947568838\n",
      "f3li42g0j0-algo-1-0f1av  | 2025-09-27T04:32:48,839 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Backend received inference at: 1758947568\n",
      "f3li42g0j0-algo-1-0f1av  | 2025-09-27T04:32:48,839 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Received content_type: application/json\n",
      "f3li42g0j0-algo-1-0f1av  | 2025-09-27T04:32:48,839 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Received content_type: application/json\n",
      "f3li42g0j0-algo-1-0f1av  | 2025-09-27T04:32:48,839 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Starting prediction\n",
      "f3li42g0j0-algo-1-0f1av  | 2025-09-27T04:32:48,840 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Starting prediction\n",
      "f3li42g0j0-algo-1-0f1av  | 2025-09-27T04:32:48,840 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Processing 1 text(s)\n",
      "f3li42g0j0-algo-1-0f1av  | 2025-09-27T04:32:48,840 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Processing 1 text(s)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f3li42g0j0-algo-1-0f1av  | 2025-09-27T04:32:49,063 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Prediction completed successfully\n",
      "f3li42g0j0-algo-1-0f1av  | 2025-09-27T04:32:49,064 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Prediction completed successfully\n",
      "f3li42g0j0-algo-1-0f1av  | 2025-09-27T04:32:49,064 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Formatting output with accept: application/json\n",
      "f3li42g0j0-algo-1-0f1av  | 2025-09-27T04:32:49,064 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Formatting output with accept: application/json\n",
      "f3li42g0j0-algo-1-0f1av  | 2025-09-27T04:32:49,064 [INFO ] W-9000-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - result=[METRICS]PredictionTime.Milliseconds:225.18|#ModelName:model,Level:Model|#type:GAUGE|#hostname:27c38f57aa3a,1758947569,1c5b321e-d315-407c-9510-f75ba40755ea, pattern=[METRICS]\n",
      "f3li42g0j0-algo-1-0f1av  | 2025-09-27T04:32:49,065 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.BatchAggregator - Sending response for jobId 1c5b321e-d315-407c-9510-f75ba40755ea\n",
      "f3li42g0j0-algo-1-0f1av  | 2025-09-27T04:32:49,065 [INFO ] W-9000-model_1.0-stdout MODEL_METRICS - PredictionTime.ms:225.18|#ModelName:model,Level:Model|#hostname:27c38f57aa3a,requestID:1c5b321e-d315-407c-9510-f75ba40755ea,timestamp:1758947569\n",
      "f3li42g0j0-algo-1-0f1av  | 2025-09-27T04:32:49,065 [INFO ] W-9000-model_1.0 ACCESS_LOG - /172.18.0.1:33512 \"POST /invocations HTTP/1.1\" 200 230\n",
      "f3li42g0j0-algo-1-0f1av  | 2025-09-27T04:32:49,066 [INFO ] W-9000-model_1.0 TS_METRICS - Requests2XX.Count:1.0|#Level:Host|#hostname:27c38f57aa3a,timestamp:1758947569\n",
      "f3li42g0j0-algo-1-0f1av  | 2025-09-27T04:32:49,066 [INFO ] W-9000-model_1.0 TS_METRICS - ts_inference_latency_microseconds.Microseconds:228028.283|#model_name:model,model_version:default|#hostname:27c38f57aa3a,timestamp:1758947569\n",
      "f3li42g0j0-algo-1-0f1av  | 2025-09-27T04:32:49,066 [INFO ] W-9000-model_1.0 TS_METRICS - ts_queue_latency_microseconds.Microseconds:145.866|#model_name:model,model_version:default|#hostname:27c38f57aa3a,timestamp:1758947569\n",
      "f3li42g0j0-algo-1-0f1av  | 2025-09-27T04:32:49,067 [INFO ] W-9000-model_1.0 TS_METRICS - QueueTime.Milliseconds:0.0|#Level:Host|#hostname:27c38f57aa3a,timestamp:1758947569\n",
      "f3li42g0j0-algo-1-0f1av  | 2025-09-27T04:32:49,067 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 227\n",
      "f3li42g0j0-algo-1-0f1av  | 2025-09-27T04:32:49,067 [INFO ] W-9000-model_1.0 TS_METRICS - WorkerThreadTime.Milliseconds:3.0|#Level:Host|#hostname:27c38f57aa3a,timestamp:1758947569\n"
     ]
    }
   ],
   "source": [
    "local_predictor.serializer = JSONSerializer()\n",
    "local_predictor.deserializer = JSONDeserializer()\n",
    "\n",
    "result = local_predictor.predict({\"texts\": \"ê¹€ì¹˜ì°Œê°œì™€ ëœì¥ì°Œê°œëŠ” í•œêµ­ì˜ ëŒ€í‘œ ìŒì‹ì…ë‹ˆë‹¤.\"})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape:  (1, 768)\n",
      "embedding_dim: 768\n",
      "num_texts:  1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def print_result(result):\n",
    "    embedding_dim = result[\"embedding_dim\"]\n",
    "    num_texts = result[\"num_texts\"]\n",
    "\n",
    "    print(\"shape: \", np.shape(result[\"embeddings\"]))\n",
    "    print(f\"embedding_dim: {embedding_dim}\")\n",
    "    print(\"num_texts: \", num_texts)\n",
    "\n",
    "print_result(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "\n",
    "def test_similarity(predictor, query, documents, top_k=3):\n",
    "    \"\"\"\n",
    "    ì¿¼ë¦¬ì™€ ë¬¸ì„œë“¤ ê°„ì˜ ìœ ì‚¬ë„ë¥¼ ê³„ì‚°í•˜ê³  ê²°ê³¼ë¥¼ ì¶œë ¥í•©ë‹ˆë‹¤.\n",
    "    \n",
    "    Args:\n",
    "        predictor: SageMaker predictor ê°ì²´\n",
    "        query (str): ê²€ìƒ‰ ì¿¼ë¦¬\n",
    "        documents (list): ë¹„êµí•  ë¬¸ì„œë“¤ì˜ ë¦¬ìŠ¤íŠ¸\n",
    "        top_k (int): ìƒìœ„ ëª‡ ê°œì˜ ë¬¸ì„œë¥¼ ì¶œë ¥í• ì§€ (ê¸°ë³¸ê°’: 3)\n",
    "    \"\"\"\n",
    "    def cosine_similarity(query_emb, doc_embs):\n",
    "        \"\"\"ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°\"\"\"\n",
    "        query_emb = np.array(query_emb)\n",
    "        doc_embs = np.array(doc_embs)\n",
    "        \n",
    "        # ì •ê·œí™”\n",
    "        query_norm = query_emb / np.linalg.norm(query_emb, axis=-1, keepdims=True)\n",
    "        doc_norms = doc_embs / np.linalg.norm(doc_embs, axis=-1, keepdims=True)\n",
    "        \n",
    "        # ì½”ì‚¬ì¸ ìœ ì‚¬ë„\n",
    "        return np.dot(query_norm, doc_norms.T).squeeze(0)\n",
    "    \n",
    "    # í—¤ë” ì¶œë ¥\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"ğŸ” ì¿¼ë¦¬: '{query}'\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # ì„ë² ë”© ë³€í™˜\n",
    "    start = time.time()\n",
    "    query_result = predictor.predict({\"texts\": query})\n",
    "    query_emb = query_result[\"embeddings\"]\n",
    "    \n",
    "    docs_result = predictor.predict({\"texts\": documents})\n",
    "    doc_embs = docs_result[\"embeddings\"]\n",
    "    encode_time = (time.time() - start) * 1000\n",
    "    \n",
    "    print(f\"\\nâœ… ì„ë² ë”© ì™„ë£Œ\")\n",
    "    print(f\"   - ì¿¼ë¦¬ ì„ë² ë”©: {np.shape(query_emb)}\")\n",
    "    print(f\"   - ë¬¸ì„œ ì„ë² ë”©: {np.shape(doc_embs)}\")\n",
    "    print(f\"   - ì¸ì½”ë”© ì‹œê°„: {encode_time:.1f}ms\")\n",
    "    \n",
    "    # ìœ ì‚¬ë„ ê³„ì‚°\n",
    "    similarities = cosine_similarity(query_emb, doc_embs)\n",
    "    \n",
    "    # ì „ì²´ ê²°ê³¼ ì¶œë ¥\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"ğŸ“Š ìœ ì‚¬ë„ ê²°ê³¼:\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    for idx, (doc, score) in enumerate(zip(documents, similarities), 1):\n",
    "        bar = \"â–ˆ\" * int(score * 30)\n",
    "        print(f\"\\n{idx}. [{score:.4f}] {bar}\")\n",
    "        print(f\"   {doc}\")\n",
    "    \n",
    "    # ìƒìœ„ Kê°œ ë¬¸ì„œ ì¶”ì¶œ\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(f\"ğŸ† ìƒìœ„ {top_k}ê°œ ë¬¸ì„œ:\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    top_k_indices = np.argsort(similarities)[::-1][:top_k]\n",
    "    for rank, idx in enumerate(top_k_indices, 1):\n",
    "        score = similarities[idx]\n",
    "        print(f\"\\n{rank}ìœ„. [{score:.4f}] {documents[idx]}\")\n",
    "    \n",
    "    print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RootlessDocker not detected, falling back to remote host IP or localhost.\n",
      "RootlessDocker not detected, falling back to remote host IP or localhost.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "ğŸ” ì¿¼ë¦¬: 'ë§›ìˆëŠ” í•œêµ­ ìŒì‹'\n",
      "======================================================================\n",
      "f3li42g0j0-algo-1-0f1av  | 2025-09-27T04:32:49,150 [INFO ] epollEventLoopGroup-3-2 TS_METRICS - ts_inference_requests_total.Count:1.0|#model_name:model,model_version:default|#hostname:27c38f57aa3a,timestamp:1758947569\n",
      "f3li42g0j0-algo-1-0f1av  | 2025-09-27T04:32:49,150 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1758947569150\n",
      "f3li42g0j0-algo-1-0f1av  | 2025-09-27T04:32:49,151 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Backend received inference at: 1758947569\n",
      "f3li42g0j0-algo-1-0f1av  | 2025-09-27T04:32:49,152 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Received content_type: application/json\n",
      "f3li42g0j0-algo-1-0f1av  | 2025-09-27T04:32:49,152 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Received content_type: application/json\n",
      "f3li42g0j0-algo-1-0f1av  | 2025-09-27T04:32:49,152 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Starting prediction\n",
      "f3li42g0j0-algo-1-0f1av  | 2025-09-27T04:32:49,152 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Starting prediction\n",
      "f3li42g0j0-algo-1-0f1av  | 2025-09-27T04:32:49,152 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Processing 1 text(s)\n",
      "f3li42g0j0-algo-1-0f1av  | 2025-09-27T04:32:49,152 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Processing 1 text(s)\n",
      "f3li42g0j0-algo-1-0f1av  | 2025-09-27T04:32:49,159 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Prediction completed successfully\n",
      "f3li42g0j0-algo-1-0f1av  | 2025-09-27T04:32:49,159 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Prediction completed successfully\n",
      "f3li42g0j0-algo-1-0f1av  | 2025-09-27T04:32:49,159 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Formatting output with accept: application/json\n",
      "f3li42g0j0-algo-1-0f1av  | 2025-09-27T04:32:49,159 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Formatting output with accept: application/json\n",
      "f3li42g0j0-algo-1-0f1av  | 2025-09-27T04:32:49,159 [INFO ] W-9000-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - result=[METRICS]PredictionTime.Milliseconds:7.9|#ModelName:model,Level:Model|#type:GAUGE|#hostname:27c38f57aa3a,1758947569,e5c564f2-b720-4c82-8d36-0e886a6743bb, pattern=[METRICS]\n",
      "f3li42g0j0-algo-1-0f1av  | 2025-09-27T04:32:49,159 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.BatchAggregator - Sending response for jobId e5c564f2-b720-4c82-8d36-0e886a6743bb\n",
      "f3li42g0j0-algo-1-0f1av  | 2025-09-27T04:32:49,160 [INFO ] W-9000-model_1.0-stdout MODEL_METRICS - PredictionTime.ms:7.9|#ModelName:model,Level:Model|#hostname:27c38f57aa3a,requestID:e5c564f2-b720-4c82-8d36-0e886a6743bb,timestamp:1758947569\n",
      "f3li42g0j0-algo-1-0f1av  | 2025-09-27T04:32:49,160 [INFO ] W-9000-model_1.0 ACCESS_LOG - /172.18.0.1:33512 \"POST /invocations HTTP/1.1\" 200 10\n",
      "f3li42g0j0-algo-1-0f1av  | 2025-09-27T04:32:49,160 [INFO ] W-9000-model_1.0 TS_METRICS - Requests2XX.Count:1.0|#Level:Host|#hostname:27c38f57aa3a,timestamp:1758947569\n",
      "f3li42g0j0-algo-1-0f1av  | 2025-09-27T04:32:49,160 [INFO ] W-9000-model_1.0 TS_METRICS - ts_inference_latency_microseconds.Microseconds:9381.62|#model_name:model,model_version:default|#hostname:27c38f57aa3a,timestamp:1758947569\n",
      "f3li42g0j0-algo-1-0f1av  | 2025-09-27T04:32:49,160 [INFO ] W-9000-model_1.0 TS_METRICS - ts_queue_latency_microseconds.Microseconds:80.293|#model_name:model,model_version:default|#hostname:27c38f57aa3a,timestamp:1758947569\n",
      "f3li42g0j0-algo-1-0f1av  | 2025-09-27T04:32:49,160 [INFO ] W-9000-model_1.0 TS_METRICS - QueueTime.Milliseconds:0.0|#Level:Host|#hostname:27c38f57aa3a,timestamp:1758947569\n",
      "f3li42g0j0-algo-1-0f1av  | 2025-09-27T04:32:49,160 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 8\n",
      "f3li42g0j0-algo-1-0f1av  | 2025-09-27T04:32:49,161 [INFO ] W-9000-model_1.0 TS_METRICS - WorkerThreadTime.Milliseconds:3.0|#Level:Host|#hostname:27c38f57aa3a,timestamp:1758947569\n",
      "f3li42g0j0-algo-1-0f1av  | 2025-09-27T04:32:49,224 [INFO ] epollEventLoopGroup-3-2 TS_METRICS - ts_inference_requests_total.Count:1.0|#model_name:model,model_version:default|#hostname:27c38f57aa3a,timestamp:1758947569\n",
      "f3li42g0j0-algo-1-0f1av  | 2025-09-27T04:32:49,224 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1758947569224\n",
      "f3li42g0j0-algo-1-0f1av  | 2025-09-27T04:32:49,225 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Backend received inference at: 1758947569\n",
      "f3li42g0j0-algo-1-0f1av  | 2025-09-27T04:32:49,225 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Received content_type: application/json\n",
      "f3li42g0j0-algo-1-0f1av  | 2025-09-27T04:32:49,226 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Received content_type: application/json\n",
      "f3li42g0j0-algo-1-0f1av  | 2025-09-27T04:32:49,226 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Starting prediction\n",
      "f3li42g0j0-algo-1-0f1av  | 2025-09-27T04:32:49,226 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Starting prediction\n",
      "f3li42g0j0-algo-1-0f1av  | 2025-09-27T04:32:49,226 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Processing 6 text(s)\n",
      "f3li42g0j0-algo-1-0f1av  | 2025-09-27T04:32:49,226 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Processing 6 text(s)\n",
      "f3li42g0j0-algo-1-0f1av  | 2025-09-27T04:32:49,238 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Prediction completed successfully\n",
      "f3li42g0j0-algo-1-0f1av  | 2025-09-27T04:32:49,239 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Prediction completed successfully\n",
      "f3li42g0j0-algo-1-0f1av  | 2025-09-27T04:32:49,239 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Formatting output with accept: application/json\n",
      "f3li42g0j0-algo-1-0f1av  | 2025-09-27T04:32:49,239 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Formatting output with accept: application/json\n",
      "f3li42g0j0-algo-1-0f1av  | 2025-09-27T04:32:49,241 [INFO ] W-9000-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - result=[METRICS]PredictionTime.Milliseconds:15.55|#ModelName:model,Level:Model|#type:GAUGE|#hostname:27c38f57aa3a,1758947569,028c247d-f08f-4a61-8d9d-078baab151cc, pattern=[METRICS]\n",
      "f3li42g0j0-algo-1-0f1av  | 2025-09-27T04:32:49,241 [INFO ] W-9000-model_1.0-stdout MODEL_METRICS - PredictionTime.ms:15.55|#ModelName:model,Level:Model|#hostname:27c38f57aa3a,requestID:028c247d-f08f-4a61-8d9d-078baab151cc,timestamp:1758947569\n",
      "f3li42g0j0-algo-1-0f1av  | 2025-09-27T04:32:49,241 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.BatchAggregator - Sending response for jobId 028c247d-f08f-4a61-8d9d-078baab151cc\n",
      "f3li42g0j0-algo-1-0f1av  | 2025-09-27T04:32:49,242 [INFO ] W-9000-model_1.0 ACCESS_LOG - /172.18.0.1:33512 \"POST /invocations HTTP/1.1\" 200 18\n",
      "f3li42g0j0-algo-1-0f1av  | 2025-09-27T04:32:49,242 [INFO ] W-9000-model_1.0 TS_METRICS - Requests2XX.Count:1.0|#Level:Host|#hostname:27c38f57aa3a,timestamp:1758947569\n",
      "f3li42g0j0-algo-1-0f1av  | 2025-09-27T04:32:49,243 [INFO ] W-9000-model_1.0 TS_METRICS - ts_inference_latency_microseconds.Microseconds:17426.244|#model_name:model,model_version:default|#hostname:27c38f57aa3a,timestamp:1758947569\n",
      "f3li42g0j0-algo-1-0f1av  | 2025-09-27T04:32:49,243 [INFO ] W-9000-model_1.0 TS_METRICS - ts_queue_latency_microseconds.Microseconds:80.933|#model_name:model,model_version:default|#hostname:27c38f57aa3a,timestamp:1758947569\n",
      "f3li42g0j0-algo-1-0f1av  | 2025-09-27T04:32:49,243 [INFO ] W-9000-model_1.0 TS_METRICS - QueueTime.Milliseconds:0.0|#Level:Host|#hostname:27c38f57aa3a,timestamp:1758947569\n",
      "f3li42g0j0-algo-1-0f1av  | 2025-09-27T04:32:49,243 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 17\n",
      "f3li42g0j0-algo-1-0f1av  | 2025-09-27T04:32:49,243 [INFO ] W-9000-model_1.0 TS_METRICS - WorkerThreadTime.Milliseconds:2.0|#Level:Host|#hostname:27c38f57aa3a,timestamp:1758947569\n",
      "\n",
      "âœ… ì„ë² ë”© ì™„ë£Œ\n",
      "   - ì¿¼ë¦¬ ì„ë² ë”©: (1, 768)\n",
      "   - ë¬¸ì„œ ì„ë² ë”©: (6, 768)\n",
      "   - ì¸ì½”ë”© ì‹œê°„: 156.7ms\n",
      "\n",
      "======================================================================\n",
      "ğŸ“Š ìœ ì‚¬ë„ ê²°ê³¼:\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "1. [0.8110] â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n",
      "   ê¹€ì¹˜ì°Œê°œì™€ ëœì¥ì°Œê°œëŠ” í•œêµ­ì˜ ëŒ€í‘œ ìŒì‹ì…ë‹ˆë‹¤.\n",
      "\n",
      "2. [0.7367] â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n",
      "   ì„œìš¸ ê°•ë‚¨ì˜ ìœ ëª…í•œ í•œì‹ë‹¹ì„ ì†Œê°œí•©ë‹ˆë‹¤.\n",
      "\n",
      "3. [0.7584] â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n",
      "   ì¼ë³¸ ë¼ë©˜ê³¼ ì´ˆë°¥ì€ ì¸ê¸°ê°€ ë§ìŠµë‹ˆë‹¤.\n",
      "\n",
      "4. [0.6628] â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n",
      "   íŒŒë¦¬ì˜ ì—í íƒ‘ì€ í”„ë‘ìŠ¤ì˜ ìƒì§•ì…ë‹ˆë‹¤.\n",
      "\n",
      "5. [0.6660] â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n",
      "   ì‚¼ì„±ì „ì ì£¼ê°€ê°€ ìƒìŠ¹í–ˆìŠµë‹ˆë‹¤.\n",
      "\n",
      "6. [0.5970] â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n",
      "   ì–‘ì ì»´í“¨í„°ì˜ ë°œì „ì´ ê°€ì†í™”ë˜ê³  ìˆìŠµë‹ˆë‹¤.\n",
      "\n",
      "======================================================================\n",
      "ğŸ† ìƒìœ„ 3ê°œ ë¬¸ì„œ:\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "1ìœ„. [0.8110] ê¹€ì¹˜ì°Œê°œì™€ ëœì¥ì°Œê°œëŠ” í•œêµ­ì˜ ëŒ€í‘œ ìŒì‹ì…ë‹ˆë‹¤.\n",
      "\n",
      "2ìœ„. [0.7584] ì¼ë³¸ ë¼ë©˜ê³¼ ì´ˆë°¥ì€ ì¸ê¸°ê°€ ë§ìŠµë‹ˆë‹¤.\n",
      "\n",
      "3ìœ„. [0.7367] ì„œìš¸ ê°•ë‚¨ì˜ ìœ ëª…í•œ í•œì‹ë‹¹ì„ ì†Œê°œí•©ë‹ˆë‹¤.\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# í…ŒìŠ¤íŠ¸ ë°ì´í„° ì •ì˜\n",
    "query = \"ë§›ìˆëŠ” í•œêµ­ ìŒì‹\"\n",
    "\n",
    "documents = [\n",
    "    \"ê¹€ì¹˜ì°Œê°œì™€ ëœì¥ì°Œê°œëŠ” í•œêµ­ì˜ ëŒ€í‘œ ìŒì‹ì…ë‹ˆë‹¤.\",        # ë§¤ìš° ê´€ë ¨\n",
    "    \"ì„œìš¸ ê°•ë‚¨ì˜ ìœ ëª…í•œ í•œì‹ë‹¹ì„ ì†Œê°œí•©ë‹ˆë‹¤.\",             # ê´€ë ¨\n",
    "    \"ì¼ë³¸ ë¼ë©˜ê³¼ ì´ˆë°¥ì€ ì¸ê¸°ê°€ ë§ìŠµë‹ˆë‹¤.\",                # ì•½ê°„ ê´€ë ¨ (ìŒì‹)\n",
    "    \"íŒŒë¦¬ì˜ ì—í íƒ‘ì€ í”„ë‘ìŠ¤ì˜ ìƒì§•ì…ë‹ˆë‹¤.\",               # ë¬´ê´€ (ê´€ê´‘)\n",
    "    \"ì‚¼ì„±ì „ì ì£¼ê°€ê°€ ìƒìŠ¹í–ˆìŠµë‹ˆë‹¤.\",                     # ì™„ì „ ë¬´ê´€ (ê²½ì œ)\n",
    "    \"ì–‘ì ì»´í“¨í„°ì˜ ë°œì „ì´ ê°€ì†í™”ë˜ê³  ìˆìŠµë‹ˆë‹¤.\",          # ì™„ì „ ë¬´ê´€ (ê¸°ìˆ )\n",
    "]\n",
    "\n",
    "# ìœ ì‚¬ë„ í…ŒìŠ¤íŠ¸ ì‹¤í–‰\n",
    "test_similarity(local_predictor, query, documents, top_k=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ì—”ë“œí¬ì¸íŠ¸ ì œê±°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gracefully Stopping... press Ctrl+C again to force\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Endpoint ì‚­ì œ ì™„ë£Œ\n"
     ]
    }
   ],
   "source": [
    "local_predictor.delete_endpoint(delete_endpoint_config=True)\n",
    "print(\"âœ… Endpoint ì‚­ì œ ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
